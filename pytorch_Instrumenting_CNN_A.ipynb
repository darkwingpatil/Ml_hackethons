{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/darkwingpatil/Ml_hackethons/blob/main/pytorch_Instrumenting_CNN_A.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l-lz7-0HC24p"
      },
      "source": [
        "# Advanced Certification in AIML\n",
        "## A Program by IIIT-H and TalentSprint"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vmx1zlGdWoUw"
      },
      "source": [
        "### Learning Objectives\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "enYFutAiiWED"
      },
      "source": [
        "At the end of the experiment, you will be able to:\n",
        "\n",
        "* classify the MNIST data using CNN\n",
        "* understand the importance of Gradient descent algorithm"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9OerY5NgyXqn",
        "cellView": "form"
      },
      "source": [
        "#@title Experiment Explanation Video\n",
        "from IPython.display import HTML\n",
        "\n",
        "HTML(\"\"\"<video width=\"850\" height=\"480\" controls>\n",
        "  <source src=\"https://cdn.talentsprint.com/talentsprint/archives/sc/aiml/aiml_batch_15/preview_videos/Instrumenting_CNN.mp4\" type=\"video/mp4\">\n",
        "</video>\n",
        "\"\"\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2hCJasxI__be"
      },
      "source": [
        "### Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zzWueA91AA7q"
      },
      "source": [
        "#### Description\n",
        "\n",
        "\n",
        "1. The dataset contains 60,000 Handwritten digits as training samples and 10,000 Test samples,\n",
        "which means each digit occurs 6000 times in the training set and 1000 times in the testing set.\n",
        "2. Each image is Size Normalized and Centered\n",
        "3. Each image is 28 X 28 Pixel with 0-255 Gray Scale Value.\n",
        "4. That means each image is represented as 784 (28 X28) dimension vector where each value is in the range 0- 255."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k5Eb8UcIE4r2"
      },
      "source": [
        "#### History\n",
        "\n",
        "Yann LeCun (Director of AI Research, Facebook, Courant Institute, NYU) was given the task of identifying the cheque numbers (in the 90’s) and the amount associated with that cheque without manual intervention. That is when this dataset was created which raised the bars and became a benchmark.\n",
        "\n",
        "Yann LeCun and Corinna Cortes (Google Labs, New York) hold the copyright of MNIST dataset, which is a subset of the original NIST datasets. This dataset is made available under the terms of the Creative Commons Attribution-Share Alike 3.0 license.\n",
        "\n",
        "It is the handwritten digits dataset in which half of them are written by the Census Bureau employees and remaining by the high school students. The digits collected among the Census Bureau employees are easier and cleaner to recognize than the digits collected among the students.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AqAHNnh3E7C0"
      },
      "source": [
        "#### Challenges\n",
        "\n",
        "Now, if you notice the images below, you will find that between 2 characters there are always certain similarities and differences. To teach a machine to recognize these patterns and identify the correct output.\n",
        "\n",
        "![altxt](https://www.researchgate.net/profile/Radu_Tudor_Ionescu/publication/282924675/figure/fig3/AS:319968869666820@1453297931093/A-random-sample-of-6-handwritten-digits-from-the-MNIST-data-set-before-and-after.png)\n",
        "\n",
        "Hence, all these challenges make this a good problem to solve in Machine Learning."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A9O2_IcPiNYV"
      },
      "source": [
        "### Domain Information"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_X-e2l14iXvo"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "Handwriting changes person to person. Some of us have neat handwriting and some have illegible handwriting such as doctors. However, if you think about it even a child who recognizes alphabets and numerics can identify the characters of a text even written by a stranger. But even a technically knowledgeable adult cannot describe the process by which he or she recognizes the text/letters. As you know this is an excellent challenge for Machine Learning.\n",
        "\n",
        "![altxt](https://i.pinimg.com/originals/f2/7a/ac/f27aac4542c0090872110836d65f4c99.jpg)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bwhk9hAX_Weh"
      },
      "source": [
        "### AI/ML Technique"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7xMc9ipI_ZZ-"
      },
      "source": [
        "A neural network is a system of interconnected artificial “neurons” that\n",
        "exchange messages between each other. The connections have numeric weights\n",
        "that are tuned during the training process, so that a properly trained network will\n",
        "respond correctly when presented with an image or pattern to recognize. A\n",
        "network consists of multiple layers of feature-detecting “neurons”. Each layer\n",
        "has many neurons that respond to different combinations of inputs from the\n",
        "previous layers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dM8vmjipJral"
      },
      "source": [
        "#### CNN (Convolutional Neural Network)\n",
        "\n",
        "CNN is also referred to as ConvNets. They are part of neural networks that have proven effective in areas as image classification and recognition.\n",
        "\n",
        "While building or training the CNN network we follow  below steps :\n",
        "\n",
        "1. We start with an input image.\n",
        "2. Then we try to apply filters or feature maps to the image, which gives us a convolutional layer.\n",
        "\n",
        "3. Then we break up the linearity of that image using the rectifier function. The image becomes ready for the pooling step.\n",
        "4. Once we're done with the pooling layer, we end up with a pooled feature map.\n",
        "5. Finally, we try to flatten our pooled feature map before inserting it into an artificial neural network.\n",
        "\n",
        "By following the above steps recurrently, we get the network's building blocks, like the weights and the feature maps, are trained and repeatedly altered in order for the network to reach the optimal performance. This will make the network to classify images and objects as accurately as possible.\n",
        "\n",
        "![alt text](https://cdn.talentsprint.com/aiml/Experiment_related_data/IMAGES/16.png)\n",
        "\n",
        "\n",
        "While working on the experiment you will be able to understand different layers involved in CNN's architecture and their importance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "btdbDNPstOyG"
      },
      "source": [
        "In this experiment, we build a neural network consisting of convolutional, pooling and fully connected layers to classify handwritten digits of the MNIST dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DlEtqiEzC24u"
      },
      "source": [
        "As we learned during the lecture sessions, CNN programming involves the following steps:\n",
        "\n",
        "1. Load the data\n",
        "2. Specify a Neural Network Model\n",
        "3. Specify the loss function and optimizer\n",
        "4. Train the model and compute the accuracy on the training dataset\n",
        "5. Compute the accuracy on the testing dataset\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qUmaxUW-rpxU"
      },
      "source": [
        "### Setup Steps:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XcrXxhfmrpxg"
      },
      "source": [
        "#@title Please enter your registration id to start: { run: \"auto\", display-mode: \"form\" }\n",
        "Id = \"2304145\" #@param {type:\"string\"}"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iDAfm706rpxg"
      },
      "source": [
        "#@title Please enter your password (normally your phone number) to continue: { run: \"auto\", display-mode: \"form\" }\n",
        "password = \"7892449987\" #@param {type:\"string\"}"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "KvdKENTWrpxh",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "06556cd0-2dc9-451d-f50e-80cbc8c0bdb5"
      },
      "source": [
        "#@title Run this cell to complete the setup for this Notebook\n",
        "from IPython import get_ipython\n",
        "import re\n",
        "ipython = get_ipython()\n",
        "\n",
        "notebook= \"U3W13_48_Instrumenting_CNN_A\" #name of the notebook\n",
        "\n",
        "def setup():\n",
        "#  ipython.magic(\"sx pip3 install torch\")\n",
        "    from IPython.display import HTML, display\n",
        "    ipython.magic(\"sx pip3 install torch\")\n",
        "    ipython.magic(\"sx pip3 install torchvision\")\n",
        "    display(HTML('<script src=\"https://dashboard.talentsprint.com/aiml/record_ip.html?traineeId={0}&recordId={1}\"></script>'.format(getId(),submission_id)))\n",
        "    print(\"Setup completed successfully\")\n",
        "    return\n",
        "\n",
        "def submit_notebook():\n",
        "    ipython.magic(\"notebook -e \"+ notebook + \".ipynb\")\n",
        "\n",
        "    import requests, json, base64, datetime\n",
        "\n",
        "    url = \"https://dashboard.talentsprint.com/xp/app/save_notebook_attempts\"\n",
        "    if not submission_id:\n",
        "      data = {\"id\" : getId(), \"notebook\" : notebook, \"mobile\" : getPassword()}\n",
        "      r = requests.post(url, data = data)\n",
        "      r = json.loads(r.text)\n",
        "\n",
        "      if r[\"status\"] == \"Success\":\n",
        "          return r[\"record_id\"]\n",
        "      elif \"err\" in r:\n",
        "        print(r[\"err\"])\n",
        "        return None\n",
        "      else:\n",
        "        print (\"Something is wrong, the notebook will not be submitted for grading\")\n",
        "        return None\n",
        "\n",
        "    elif getAnswer() and getComplexity() and getAdditional() and getConcepts() and getWalkthrough() and getComments() and getMentorSupport():\n",
        "      f = open(notebook + \".ipynb\", \"rb\")\n",
        "      file_hash = base64.b64encode(f.read())\n",
        "\n",
        "      data = {\"complexity\" : Complexity, \"additional\" :Additional,\n",
        "              \"concepts\" : Concepts, \"record_id\" : submission_id,\n",
        "              \"answer\" : Answer, \"id\" : Id, \"file_hash\" : file_hash,\n",
        "              \"notebook\" : notebook, \"feedback_walkthrough\":Walkthrough ,\n",
        "              \"feedback_experiments_input\" : Comments,\n",
        "              \"feedback_inclass_mentor\": Mentor_support}\n",
        "\n",
        "      r = requests.post(url, data = data)\n",
        "      r = json.loads(r.text)\n",
        "      if \"err\" in r:\n",
        "        print(r[\"err\"])\n",
        "        return None\n",
        "      else:\n",
        "        print(\"Your submission is successful.\")\n",
        "        print(\"Ref Id:\", submission_id)\n",
        "        print(\"Date of submission: \", r[\"date\"])\n",
        "        print(\"Time of submission: \", r[\"time\"])\n",
        "        print(\"View your submissions: https://aiml-iiith.talentsprint.com/notebook_submissions\")\n",
        "        #print(\"For any queries/discrepancies, please connect with mentors through the chat icon in LMS dashboard.\")\n",
        "        return submission_id\n",
        "    else: submission_id\n",
        "\n",
        "\n",
        "def getAdditional():\n",
        "  try:\n",
        "    if not Additional:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Additional\n",
        "  except NameError:\n",
        "    print (\"Please answer Additional Question\")\n",
        "    return None\n",
        "\n",
        "def getComplexity():\n",
        "  try:\n",
        "    if not Complexity:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Complexity\n",
        "  except NameError:\n",
        "    print (\"Please answer Complexity Question\")\n",
        "    return None\n",
        "\n",
        "def getConcepts():\n",
        "  try:\n",
        "    if not Concepts:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Concepts\n",
        "  except NameError:\n",
        "    print (\"Please answer Concepts Question\")\n",
        "    return None\n",
        "\n",
        "\n",
        "def getWalkthrough():\n",
        "  try:\n",
        "    if not Walkthrough:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Walkthrough\n",
        "  except NameError:\n",
        "    print (\"Please answer Walkthrough Question\")\n",
        "    return None\n",
        "\n",
        "def getComments():\n",
        "  try:\n",
        "    if not Comments:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Comments\n",
        "  except NameError:\n",
        "    print (\"Please answer Comments Question\")\n",
        "    return None\n",
        "\n",
        "\n",
        "def getMentorSupport():\n",
        "  try:\n",
        "    if not Mentor_support:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Mentor_support\n",
        "  except NameError:\n",
        "    print (\"Please answer Mentor support Question\")\n",
        "    return None\n",
        "\n",
        "def getAnswer():\n",
        "  try:\n",
        "    if not Answer:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Answer\n",
        "  except NameError:\n",
        "    print (\"Please answer Question\")\n",
        "    return None\n",
        "\n",
        "\n",
        "def getId():\n",
        "  try:\n",
        "    return Id if Id else None\n",
        "  except NameError:\n",
        "    return None\n",
        "\n",
        "def getPassword():\n",
        "  try:\n",
        "    return password if password else None\n",
        "  except NameError:\n",
        "    return None\n",
        "\n",
        "submission_id = None\n",
        "### Setup\n",
        "if getPassword() and getId():\n",
        "  submission_id = submit_notebook()\n",
        "  if submission_id:\n",
        "    setup()\n",
        "else:\n",
        "  print (\"Please complete Id and Password cells before running setup\")\n",
        "\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<script src=\"https://dashboard.talentsprint.com/aiml/record_ip.html?traineeId=2304145&recordId=5960\"></script>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setup completed successfully\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bikcf-BRC24z"
      },
      "source": [
        "### Importing required packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3GuIxHsqC240"
      },
      "source": [
        "# Importing Pytorch library\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "# Matplotlib is used for ploting graphs\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EUarXxbEC246"
      },
      "source": [
        "### Loading the data\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s0wMGecqu5Ic"
      },
      "source": [
        "The database contains 60,000 training images and 10,000 testing images each of size 28x28. Loading the dataset can be easily done through the torch.utils package. The dataset is downloaded automatically when you run the below cell for the first time."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CWvuH-yKDv_1"
      },
      "source": [
        "# Normalize with mean and std ( 0.1307 and 0.3081 are the mean and std of MNIST data )\n",
        "transform = transforms.Compose([transforms.ToTensor(),transforms.Normalize((0.1307,), (0.3081,))])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ENEeWQzZD0x2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "10387178-8fec-4ada-b317-7243031799f9"
      },
      "source": [
        "# Loading the train set file\n",
        "mnist_train = datasets.MNIST(root='../data',\n",
        "                            train=True,\n",
        "                            transform=transform,\n",
        "                            download=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ../data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9912422/9912422 [00:00<00:00, 16875238.71it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ../data/MNIST/raw/train-images-idx3-ubyte.gz to ../data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ../data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 469990.00it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ../data/MNIST/raw/train-labels-idx1-ubyte.gz to ../data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ../data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1648877/1648877 [00:00<00:00, 4237540.94it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ../data/MNIST/raw/t10k-images-idx3-ubyte.gz to ../data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ../data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 4003894.23it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ../data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ../data/MNIST/raw\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ssNtVBHvFOgL"
      },
      "source": [
        "# Loading the test set file\n",
        "mnist_test = datasets.MNIST(root='../data',\n",
        "                           train=False,\n",
        "                           transform=transform)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bmxkoOr_v6oh"
      },
      "source": [
        " Let’s visualize a few data from the training set to get a better idea about the purpose of using the deep learning model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ywedmu7Rv0dJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7b4b9115-0126-4b7a-b1f7-4a256eb4457b"
      },
      "source": [
        "# Plotting one example\n",
        "print(\"Shape of the training data (no of images, height, width) : \", mnist_train.train_data.size()) # (60000, 28, 28)\n",
        "print(\"Shape of the testing data (no of images, height, width) : \", mnist_test.test_data.size())  # (10000, 28, 28)\n",
        "\n",
        "# YOUR CODE HERE : Plot one image from the training set as a gray scale image."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of the training data (no of images, height, width) :  torch.Size([60000, 28, 28])\n",
            "Shape of the testing data (no of images, height, width) :  torch.Size([10000, 28, 28])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/datasets/mnist.py:76: UserWarning: train_data has been renamed data\n",
            "  warnings.warn(\"train_data has been renamed data\")\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/datasets/mnist.py:81: UserWarning: test_data has been renamed data\n",
            "  warnings.warn(\"test_data has been renamed data\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z2--teHtC25E"
      },
      "source": [
        "#### Minibatch\n",
        "The Machine learning dataset can be really large. Hence we cannot often load the entire data into the memory. Hence neural network training is done by loading small batches (commonly called minibatch) of data and using it to update the learnable parameters (weights and biases) of the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4NDlkt8UGQGX"
      },
      "source": [
        "# The mini batch size used for training\n",
        "batch_size = 1000\n",
        "\n",
        "# Loading the train dataset\n",
        "# Data Loader loads the images and corresponding labels of defined mini batch size.\n",
        "# the image batch shape will be (batch_size, 1, 28, 28)\n",
        "train_loader = torch.utils.data.DataLoader(dataset=mnist_train,\n",
        "                                           batch_size=batch_size,\n",
        "                                           shuffle=True)\n",
        "\n",
        "# Loading the test dataset\n",
        "# Data loader will behave like an iterator, so we can loop over it and fetch a different mini-batch every time.\n",
        "test_loader = torch.utils.data.DataLoader(dataset=mnist_test,\n",
        "                                          batch_size=batch_size,\n",
        "                                          shuffle=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "06iKhdT5H9tI"
      },
      "source": [
        "Let’s visualize a few images in the mini batch of training set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PfXCm8nTC25F",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 766
        },
        "outputId": "8169fcbf-eb63-4b42-9e46-1ae78b058ace"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "batch_count = 0\n",
        "for mini_batch in train_loader:\n",
        "    images, labels = mini_batch\n",
        "    print('Mini batch size: images -', images.size(), ' labels - ', labels.size())\n",
        "    for j in range(5):  # Basically iterating a few times (hence range(5)) to print a few images in this mini-batch\n",
        "        print(images[j].size(), labels[j])\n",
        "        # YOUR CODE HERE : Print few images from the mini-batch. Note: You might need to convert tensor to numpy to plot it.\n",
        "        fig=plt.figure(figsize=(25,4))\n",
        "        ax = fig.add_subplot(2, int(30/2), j+1, xticks=[], yticks=[])\n",
        "        ax.imshow(np.squeeze(images[j]), cmap='gray')\n",
        "        ax.set_title((labels[j].item()))\n",
        "        # To plot only 2 images from each batch, applied logic to break out of the loop at range = 1.\n",
        "        if j == 1:\n",
        "            print(j)\n",
        "            break\n",
        "\n",
        "    # If you want to visualize images in the next mini-batches you can increase the Batch count value.\n",
        "    if batch_count == 1:\n",
        "        break\n",
        "\n",
        "    batch_count +=1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mini batch size: images - torch.Size([1000, 1, 28, 28])  labels -  torch.Size([1000])\n",
            "torch.Size([1, 28, 28]) tensor(9)\n",
            "torch.Size([1, 28, 28]) tensor(8)\n",
            "1\n",
            "Mini batch size: images - torch.Size([1000, 1, 28, 28])  labels -  torch.Size([1000])\n",
            "torch.Size([1, 28, 28]) tensor(1)\n",
            "torch.Size([1, 28, 28]) tensor(7)\n",
            "1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 2500x400 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 2500x400 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 2500x400 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAIAAAACXCAYAAADd7VPoAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAEHklEQVR4nO3doU8jQRiG8W/JJailoUGUAqEeCAqNwKCwYHGY/gWEkKCRJDiCI6BIkAgUCQJJwJAQUrIYRLuLwNA9caHX6R3HinZnj+f9qU7WfKEP00lplyBN09QEa8j3AOKXAoBTAHAKAE4BwCkAOAUApwDgFACcAoBDBvD6+mrb29u2vLxs5XLZgiCww8ND32N5gQzg5eXFdnZ27O7uzubn532P49UP3wP4MD4+bs/Pz1apVOz6+toWFhZ8j+QNcgcYHh62SqXie4xCQAYgvykAOAUApwDgFACcAoBTAHDIN4LMzPb29qzZbFoURWZmdnZ2Zk9PT2ZmVq/XrVQq+RwvNwH1Y+G1Ws0eHx//eu3h4cFqtVq+A3mCDUB+0RkATgHAKQA4BQCnAOAUAFymN4La7bZFUWRhGFoQBIOeSfogTVNLksSq1aoNDX3+e54pgCiKbGpqqm/DSX4ajYZNTk5+ej3TS0AYhn0bSPL11XOXKQBt+/+vr547HQLhFACcAoBTAHAKAE4BwCkAOAUApwDgFACcAoBTAHAKAE4BwCkAOAUAh/1y6IfFxUVnvbGx4axXV1ed9cXFhbNeWloazGA50Q4ApwDgFAAc/gwwOzvrrHtf83u/Pf9xE4nvQjsAnAKAUwBw+DPA+vq6s35/f3fWm5ubznp3d3fgM+VJOwCcAoBTAHCZbhMXx/G3unHizMxM5/Hl5aVzrdFoOOu5ublcZhqUVqtlIyMjn17XDgCnAOAUABzyfYB6vd553HsHjePj47zH8Uo7AJwCgFMAcMgzQLlc7jzuvYnS6elpztP4pR0ATgHAKQA4xBlgdHTUWXd/F+D+/t65dnNzk8tMRaEdAE4BwCkAOMQZoPfv4WNjY53HJycneY9TKNoB4BQAnAKAQ5wBej/7L79pB4BTAHCIl4C1tTXfIxSWdgA4BQCnAOAQZ4Duj4CZmfOvVPf39/Mep1C0A8ApADgFAIc4A/R+A77dbnuapHi0A8ApADgFAKcA4BQAnAKAUwBwiPcB/mViYsJZ397eeprED+0AcAoATgHAIc4AR0dHzrr7NnErKyvOtfPz81xmKgrtAHAKAE4BwCHOAM1m01l3fyaQTj8JOAUApwDgEGeA3tu/bm1t+RmkgLQDwCkAOAUAhzgDRFHke4TC0g4ApwDgFAAc4gwQx7Gzvrq68jRJ8WgHgFMAcIiXgLe3N2d9cHDQeTw9PZ33OIWiHQBOAcApALgg7b1/yl/EcWylUimPeaTPWq3WH/8xpZt2ADgFAKcA4BQAnAKAUwBwCgBOAcApADgFAJcpgAzvFktBffXcZQogSZK+DCP5++q5y/THoHa7bVEUWRiGFgRB34aTwUnT1JIksWq1+s/7IWQKQL4vHQLhFACcAoBTAHAKAE4BwCkAuJ9PkcsjJmk3UgAAAABJRU5ErkJggg==\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 2500x400 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e8aYrO1awfiV"
      },
      "source": [
        "### Defining a CNN based Neural Network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cC1rDpB5wj58"
      },
      "source": [
        "Now we will define a CNN based neural network, that takes the input as 28x28 MNIST images and predicts a label from 0 to 9. The predictions will be of the form of a probability distribution given as an array $P$ of length 10, where each entry $P_i$  denotes the probability of the input image being the digit $i$.\n",
        "\n",
        "\n",
        "We will divide the neural network into two parts. First is the feature extractor, which given the 28x28 images, gives a feature vector. The feature extractor is a CNN based neural network. Second is a classifier, which takes the feature vector as input and produces a 10 dimensional vector called the logits. Finally the logits are converted in the prediction probabilities by applying the softmax function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "epKavSwZwvFs"
      },
      "source": [
        "The Deep CNN we will be using is called LeNet. A pictorial representation is given below: </br>\n",
        "**NOTE: The diagram below assumes the image of size 32 \\* 32, however, MNIST is 28 \\* 28; So the diagram is not a representation of the problem dataset, but a LeNet architecture in general. It could however be treated as an interesting exercise for you to recompute each of the layers gives the slight change in the input dimension.**\n",
        "\n",
        "![alt text](https://cdn.talentsprint.com/aiml/Experiment_related_data/IMAGES/cnn.png)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QIgeu0AqC25L"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "As you can see, the neural network has multiple operations happening one after another. Each operation has learnable parameters (weights and biases). Typically we call them the layers of a neural network. Neural networks can have many layers, and are hence called Deep Neural Networks (DNNs) or Deep CNNs.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nxyDKtLlw6Pq"
      },
      "source": [
        "LeNet feature extractor shown above has the following layers:\n",
        "\n",
        "1. Convolutional layer which:\n",
        "    1. takes an image with 1 channel (since MNIST digits are black and white; for color images, they are represented by 3 channels giving the intensities of Red, Blue and Green) : 1x28x28\n",
        "    2. convolves with 6 filters (weights) of kernel size 5x5 and stride 1\n",
        "    3. without padding\n",
        "    4. so, gives a 6x24x24 tensor as output\n",
        "2. Subsampling or MaxPooling (which reduces the height and width by half). Here we are doing 2x2-MaxPooling which takes the maximum value of every non-overlapping 2x2 window: outputs a 6x12x12 tensor\n",
        "3. ReLU activation function applied to every entry of the tensor\n",
        "4. Convolutional layer which:\n",
        "    1. takes a tensor with 6 channels: 6x12x12\n",
        "    2. convolves with 16 filters of kernel size 5x5 and stride 1\n",
        "    3. so, gives a 16x8x8 tensor as output\n",
        "4. MaxPooling: gives a 16x4x4 tensor as output\n",
        "5. ReLU"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EMjkQOxyw0hz"
      },
      "source": [
        "Note that the output of the below neural network is a 3D tensor. This is because the input is a 3D tensor (with one dimension =1) and Convolutional and Max-Pooling layers give 3D tensors as output. Next, we will reshape this 3D tensor into a long vector and pass it through the classifier network. The Classifier network is typically a Multi-Layered Perceptron Network that you have seen previously."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4KFQv7tOC25M"
      },
      "source": [
        "# A CNN based Feature extractor\n",
        "# Defining neural network in python by a class that inherits from nn.Module\n",
        "class LeNet(nn.Module):\n",
        "    \"\"\"LeNet feature extractor model.\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        \"\"\"Init LeNet feature extractor model.\"\"\"\n",
        "        super(LeNet, self).__init__()\n",
        "\n",
        "        # Defining the CNNfeature Extractor\n",
        "        self.feature_extractor = nn.Sequential(\n",
        "            # input [1 x 28 x 28]\n",
        "            # 1st conv layer\n",
        "            # Conv which convolves input image with 6 filters of 5x5 size, without padding\n",
        "            nn.Conv2d(1, 6, kernel_size=5),\n",
        "            # [6 x 24 x 24]\n",
        "            nn.MaxPool2d(kernel_size=2), # Max pooling subsampling operation\n",
        "            # [6 x 12 x 12]\n",
        "            nn.ReLU(), # Non linear activation function\n",
        "            # 2nd conv layer\n",
        "            # input [6 x 12 x 12]\n",
        "            # Conv which convolves input image with 16 filters of 5x5 size, without padding\n",
        "            nn.Conv2d(6, 16, kernel_size=5),\n",
        "            # [16 x 8 x 8]\n",
        "            nn.MaxPool2d(kernel_size=2),\n",
        "            # [16 x 4 x 4]\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        # Defining the Classifier\n",
        "        self.classifier = nn.Sequential(\n",
        "            # Linear layer with 120 hidden nodes, taking a flattened [16 x 4 x 4] as input\n",
        "            nn.Linear(16 * 4 * 4, 120),\n",
        "            # Linear layer with 84 hidden nodes\n",
        "            nn.Linear(120, 84),\n",
        "            # ReLU\n",
        "            nn.ReLU(),\n",
        "            # Output layer with as many nodes as number of classes\n",
        "            nn.Linear(84, 10)\n",
        "        )\n",
        "\n",
        "    def forward(self, input):\n",
        "        \"\"\"Define a Forward pass of the LeNet.\"\"\"\n",
        "        out = self.feature_extractor(input) # Pass input through the feature extractor\n",
        "        out = out.view(-1, 16 * 4 * 4) # Reshape the 2D to a vector\n",
        "        out = self.classifier(out) # Pass features through the classifier to get predictions\n",
        "        # Convert the predictions to probabilities, by applying the softmax function\n",
        "        out = F.softmax(out)\n",
        "        return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vFmW4izimMaM"
      },
      "source": [
        "Every Tensor in PyTorch has a **to()** member function. Its job is to put the tensor on which it's called to a certain device whether it be the CPU or a certain GPU.\n",
        "\n",
        "Input to the to function is a torch.device object which can be initialized with either of the following inputs.\n",
        "* cpu for CPU\n",
        "* cuda:0 for putting it on GPU number 0. Similarly, if your system has multiple GPUs, then the respective number would be considered while initializing the device.\n",
        "\n",
        "Generally, whenever you initialize a Tensor, it’s put on the CPU. You should move it to the GPU to make the related calculation faster.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NeuqAK-Il3yj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c733852f-4d9d-4884-8084-cf75fcd9d745"
      },
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "device"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda', index=0)"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ElGuL0rLgSj"
      },
      "source": [
        "### Creating an instance of the network\n",
        "Let us declare an object of class LeNet, and make it a CUDA model if CUDA is available:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QTCduCI1cusJ"
      },
      "source": [
        "Next we will inspect our model, and see the parameters in each layer. Note that the activation and MaxPooling layers do not have any learnable parameters. Also note the sizes of parameters for each layer.\n",
        "\n",
        "- For convolutional layer weights, it is output_channels x input_channels x window_width x window_height.\n",
        "- For convolutional layer biases, it is output_channels.\n",
        "- For linear layer weights, it is output_size x input_size\n",
        "- For linear layer biases, it is output_size"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4uPkNRmsC25R",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cc0ebf3c-f319-4771-b7da-f08900bc423e"
      },
      "source": [
        "lenet = LeNet()\n",
        "lenet = lenet.to(device)  # Making the lenet to run on available runtime\n",
        "\n",
        "# Print out the size of parameters of each layer\n",
        "# state_dict() is simply a Python dictionary object that maps each layer to its parameter tensor\n",
        "for name, param in lenet.state_dict().items():\n",
        "    print(name, '\\n', param.size(), '\\n')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "feature_extractor.0.weight \n",
            " torch.Size([6, 1, 5, 5]) \n",
            "\n",
            "feature_extractor.0.bias \n",
            " torch.Size([6]) \n",
            "\n",
            "feature_extractor.3.weight \n",
            " torch.Size([16, 6, 5, 5]) \n",
            "\n",
            "feature_extractor.3.bias \n",
            " torch.Size([16]) \n",
            "\n",
            "classifier.0.weight \n",
            " torch.Size([120, 256]) \n",
            "\n",
            "classifier.0.bias \n",
            " torch.Size([120]) \n",
            "\n",
            "classifier.1.weight \n",
            " torch.Size([84, 120]) \n",
            "\n",
            "classifier.1.bias \n",
            " torch.Size([84]) \n",
            "\n",
            "classifier.3.weight \n",
            " torch.Size([10, 84]) \n",
            "\n",
            "classifier.3.bias \n",
            " torch.Size([10]) \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pox6ItueC25X"
      },
      "source": [
        "### Do a Forward Pass (an example), and compute the accuracy\n",
        "\n",
        "The code below randomly loops over the train-data, does forward pass and compute the accuracy on the same. The weights and biases of the network are randomly initialized by default.\n",
        "\n",
        "Hence the prediction accuracy currently is very close to a random guess of the labels which is 1/10 = 10%.\n",
        "\n",
        "The step is done as the last step in the typical deep learning program. It is given here just for illustrating Forward pass."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y7mMsccnC25Z",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 406
        },
        "outputId": "80df9b85-edd4-4295-8b26-17b9a265a4c5"
      },
      "source": [
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "for images, labels in train_loader:\n",
        "    # Convert the images and labels to gpu for faster execution\n",
        "    images = images.to(device)\n",
        "    labels = labels.to(device)\n",
        "\n",
        "    result = lenet(images)\n",
        "\n",
        "    # YOUR CODE HERE : Perform forward pass by passing 'images' as input to the model 'lenet' defined above.\n",
        "\n",
        "    # Find the prediction with the largest probability\n",
        "    _,pred = torch.max(result.data,1)\n",
        "    total += labels.size(0)\n",
        "\n",
        "    # correct is incremented by the numer of predictions which are correct (equal to the ground truth labels)\n",
        "    correct += (pred == labels).sum().item()\n",
        "\n",
        "print('Accuracy of random Train Data:', 100 * correct/total)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-34-5949c3d529ff>:48: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  out = F.softmax(out)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-37-89f0a4ed4729>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtotal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0;31m# Convert the images and labels to gpu for faster execution\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    629\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    630\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 631\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    632\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    633\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    673\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    674\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 675\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    676\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    677\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/datasets/mnist.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_transform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1532\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1539\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1540\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1543\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, tensor)\u001b[0m\n\u001b[1;32m    275\u001b[0m             \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mNormalized\u001b[0m \u001b[0mTensor\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m         \"\"\"\n\u001b[0;32m--> 277\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minplace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    278\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py\u001b[0m in \u001b[0;36mnormalize\u001b[0;34m(tensor, mean, std, inplace)\u001b[0m\n\u001b[1;32m    348\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"img should be Tensor Image. Got {type(tensor)}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 350\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mF_t\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmean\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minplace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    352\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/transforms/_functional_tensor.py\u001b[0m in \u001b[0;36mnormalize\u001b[0;34m(tensor, mean, std, inplace)\u001b[0m\n\u001b[1;32m    902\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    903\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmean\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstd\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 904\u001b[0;31m     \u001b[0m_assert_image_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    905\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    906\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/transforms/_functional_tensor.py\u001b[0m in \u001b[0;36m_assert_image_tensor\u001b[0;34m(img)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_assert_image_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_is_tensor_a_torch_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Tensor is not a torch image.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R8ZaomZlC25e"
      },
      "source": [
        "### Defining Loss Function and Optimizer\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2mzSRNhyxt_L"
      },
      "source": [
        "The **loss function** is a way of measuring the difference between the current prediction of the network and the correct prediction. As we saw in the lecture, the gradient descent algorithm is essentially adjusting the learnable parameters (weights and biases) of the network so as to decrease the loss. Here we will be using the **cross entropy loss**, which is commonly used for classification tasks (predicting a class from 0 to 9).\n",
        "\n",
        "The **learning rate** is a small fraction which is used to multiply the gradients of the loss function with respect to the weights. The idea behind doing this is that, we do not want to make drastic change to the weights of the neural network in each step, but rather a gradual one.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iQRWcL7fC25f"
      },
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        " # YOUR CODE HERE : Explore and Define a loss function\n",
        "\n",
        "learning_rate = 0.001\n",
        "# YOUR CODE HERE : Set the learning rate.\n",
        "\n",
        "optimizer = torch.optim.Adam(lenet.parameters(), lr=learning_rate)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V5vKPAKCC25k"
      },
      "source": [
        "### Training the Model\n",
        "\n",
        "\n",
        "Now that we have loaded the data, defined the neural network, specified the loss function and optimizer algorithm, we can do the training. The training is done by loading a part of the training data, called minibatch. The size of the minibatch is specified by the batch_size. We will load one minibatch at a time and do forward as well as backward pass on the model. We will keep doing things by looping over the entire dataset.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kNjXeIwM0qBv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b0f1cb7a-874a-4050-85be-9623aeba268e"
      },
      "source": [
        "# No of Epochs\n",
        "epoch = 8\n",
        "\n",
        "# First switch the module mode to lenet.train() so that new weights can be learned after every epoch.\n",
        "lenet.train()\n",
        "train_losses, train_accuracy = [], []\n",
        "\n",
        "# Loop for no of epochs\n",
        "for e in range(epoch):\n",
        "    train_loss = 0\n",
        "    correct = 0\n",
        "    # Iterate through all the batches in each epoch\n",
        "    for images, labels in train_loader:\n",
        "\n",
        "      # Convert the image and label to gpu for faster execution\n",
        "      images = images.to(device)\n",
        "      labels = labels.to(device)\n",
        "\n",
        "      # YOUR CODE HERE: Zero out the gradients using 'zero_grad' function\n",
        "\n",
        "      lenet.zero_grad()\n",
        "\n",
        "      predictions = lenet(images)\n",
        "      # YOUR CODE HERE: Perform forward pass on the current mini batch by passing 'image' as input to the model.\n",
        "      loss = criterion(predictions,labels)\n",
        "      # YOUR CODE HERE: Compute the loss using 'criterion' defined above.\n",
        "      train_loss += loss.item()\n",
        "\n",
        "      # YOUR CODE HERE: Perform backward pass\n",
        "      loss.backward()\n",
        "\n",
        "      # YOUR CODE HERE: Update the parameters using the gradients with the learning rate\n",
        "\n",
        "      optimizer.step()\n",
        "\n",
        "      # Accuracy calculation\n",
        "      _, predicted = torch.max(predictions, 1)\n",
        "      correct += (predicted == labels).sum().item()\n",
        "\n",
        "    train_losses.append(train_loss/len(mnist_train))\n",
        "    train_accuracy.append(100 * correct/len(mnist_train))\n",
        "    print('epoch: {}, Train Accuracy: {:.2f} '.format(e+1, train_accuracy[-1]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-34-5949c3d529ff>:48: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  out = F.softmax(out)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 1, Train Accuracy: 64.76 \n",
            "epoch: 2, Train Accuracy: 88.03 \n",
            "epoch: 3, Train Accuracy: 93.60 \n",
            "epoch: 4, Train Accuracy: 95.19 \n",
            "epoch: 5, Train Accuracy: 96.22 \n",
            "epoch: 6, Train Accuracy: 96.80 \n",
            "epoch: 7, Train Accuracy: 97.09 \n",
            "epoch: 8, Train Accuracy: 97.45 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j2fo6SPxC25r"
      },
      "source": [
        "### Compute the Accuracy of the Model on the Test data\n",
        "Finally, we need to check how well the model is doing on the testing data. This step is also done by loading the data one minibatch at a time and computing the accuracy, which is finally averaged."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CKk8LeBdC25t",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "62a45d07-4007-4719-d3bf-3087a69d84cb"
      },
      "source": [
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "for images, labels in test_loader:\n",
        "\n",
        "    # Convert images and labels to gpu runtime for faster execution\n",
        "    images = images.to(device)\n",
        "    labels = labels.to(device)\n",
        "\n",
        "    # Passing the data to the model (Forward Pass)\n",
        "    result = lenet(images)\n",
        "\n",
        "    _,pred = torch.max(predictions, 1)\n",
        "\n",
        "    total += labels.size(0)\n",
        "\n",
        "    # correct is incremented by the numer of prediction which are correct (equal to the ground truth labels)\n",
        "    correct += (pred == labels).sum().item()\n",
        "\n",
        "print(\"Accuracy of Test Data: {0:.2f}%\".format(correct/total *100))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-34-5949c3d529ff>:48: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  out = F.softmax(out)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of Test Data: 10.27%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CA-4A-s6HbfL"
      },
      "source": [
        "**Ungraded Exercise:** Try with different values of the learning rate and see how it affects the training. You should observe that if the learning rate is very small, the model hardly learns (that produces less accuracy).\n",
        "\n",
        "Finding the optimal learning rate is often a trial and error method."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VHfHdGCP_n6Y"
      },
      "source": [
        "### Please answer the questions below to complete the experiment:\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kmvdJ4aNmGjR"
      },
      "source": [
        "#@title During forward pass when \"result = lenet(images)\" is called, the variable 'result' holds output of the forward() method of LeNet class? (You may refer the following [link](https://discuss.pytorch.org/t/confusion-regarding-use-of-forward-function-while-developing-a-cnn-model/52058/2)) { run: \"auto\", form-width: \"500px\", display-mode: \"form\" }\n",
        "Answer = \"TRUE\" #@param [\"\",\"TRUE\", \"FALSE\"]\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nmufjlR-rjv6"
      },
      "source": [
        "#@title How was the experiment? { run: \"auto\", form-width: \"500px\", display-mode: \"form\" }\n",
        "Complexity = \"Good and Challenging for me\" #@param [\"\",\"Too Simple, I am wasting time\", \"Good, But Not Challenging for me\", \"Good and Challenging for me\", \"Was Tough, but I did it\", \"Too Difficult for me\"]\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MZ7vVuShrjv6"
      },
      "source": [
        "#@title If it was too easy, what more would you have liked to be added? If it was very difficult, what would you have liked to have been removed? { run: \"auto\", display-mode: \"form\" }\n",
        "Additional = \"NA\" #@param {type:\"string\"}\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4VBk_4VTAxCM"
      },
      "source": [
        "#@title Can you identify the concepts from the lecture which this experiment covered? { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n",
        "Concepts = \"Yes\" #@param [\"\",\"Yes\", \"No\"]\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r35isHfTVGKc"
      },
      "source": [
        "#@title  Experiment walkthrough video? { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n",
        "Walkthrough = \"Didn't use\" #@param [\"\",\"Very Useful\", \"Somewhat Useful\", \"Not Useful\", \"Didn't use\"]\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XH91cL1JWH7m"
      },
      "source": [
        "#@title  Text and image description/explanation and code comments within the experiment: { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n",
        "Comments = \"Somewhat Useful\" #@param [\"\",\"Very Useful\", \"Somewhat Useful\", \"Not Useful\", \"Didn't use\"]\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z8xLqj7VWIKW"
      },
      "source": [
        "#@title Mentor Support: { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n",
        "Mentor_support = \"Didn't use\" #@param [\"\",\"Very Useful\", \"Somewhat Useful\", \"Not Useful\", \"Didn't use\"]\n"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FzAZHt1zw-Y-",
        "cellView": "form",
        "outputId": "9eb7dfb1-78eb-4b1e-dcb0-286ff81c3901",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#@title Run this cell to submit your notebook for grading { vertical-output: true }\n",
        "try:\n",
        "  if submission_id:\n",
        "      return_id = submit_notebook()\n",
        "      if return_id : submission_id = return_id\n",
        "  else:\n",
        "      print(\"Please complete the setup first.\")\n",
        "except NameError:\n",
        "  print (\"Please complete the setup first.\")"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Your submission is successful.\n",
            "Ref Id: 5960\n",
            "Date of submission:  18 Aug 2024\n",
            "Time of submission:  20:33:18\n",
            "View your submissions: https://aiml-iiith.talentsprint.com/notebook_submissions\n"
          ]
        }
      ]
    }
  ]
}