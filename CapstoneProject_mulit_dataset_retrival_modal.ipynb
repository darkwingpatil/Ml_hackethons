{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyP0jc/VYXtjmCeXC00hsmIu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/darkwingpatil/Ml_hackethons/blob/main/CapstoneProject_mulit_dataset_retrival_modal.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Multi media retrival  "
      ],
      "metadata": {
        "id": "wlQwGe9EcF54"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# For image download\n",
        "!wget https://github.com/jbrownlee/Datasets/releases/download/Flickr8k/Flickr8k_Dataset.zip\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ui0WsV6rcdTg",
        "outputId": "f54f4123-e41a-4a82-c4d3-aa2e2e1512ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-12-01 14:42:41--  https://github.com/jbrownlee/Datasets/releases/download/Flickr8k/Flickr8k_Dataset.zip\n",
            "Resolving github.com (github.com)... 140.82.113.3\n",
            "Connecting to github.com (github.com)|140.82.113.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://objects.githubusercontent.com/github-production-release-asset-2e65be/124585957/47f52b80-3501-11e9-8f49-4515a2a3339b?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=releaseassetproduction%2F20241201%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20241201T144242Z&X-Amz-Expires=300&X-Amz-Signature=8631b3262cfd05cb80f05ea1fb75dd6de293279531cbeee8e500ee416a11d740&X-Amz-SignedHeaders=host&response-content-disposition=attachment%3B%20filename%3DFlickr8k_Dataset.zip&response-content-type=application%2Foctet-stream [following]\n",
            "--2024-12-01 14:42:42--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/124585957/47f52b80-3501-11e9-8f49-4515a2a3339b?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=releaseassetproduction%2F20241201%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20241201T144242Z&X-Amz-Expires=300&X-Amz-Signature=8631b3262cfd05cb80f05ea1fb75dd6de293279531cbeee8e500ee416a11d740&X-Amz-SignedHeaders=host&response-content-disposition=attachment%3B%20filename%3DFlickr8k_Dataset.zip&response-content-type=application%2Foctet-stream\n",
            "Resolving objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115419746 (1.0G) [application/octet-stream]\n",
            "Saving to: ‘Flickr8k_Dataset.zip’\n",
            "\n",
            "Flickr8k_Dataset.zi 100%[===================>]   1.04G   233MB/s    in 4.2s    \n",
            "\n",
            "2024-12-01 14:42:46 (251 MB/s) - ‘Flickr8k_Dataset.zip’ saved [1115419746/1115419746]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# for caption dataset downlaod\n",
        "!wget https://github.com/jbrownlee/Datasets/releases/download/Flickr8k/Flickr8k_text.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E8EboKJ8g5K2",
        "outputId": "759951c7-a8af-4b33-974e-1b5297682a1f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-12-01 14:42:53--  https://github.com/jbrownlee/Datasets/releases/download/Flickr8k/Flickr8k_text.zip\n",
            "Resolving github.com (github.com)... 140.82.112.4\n",
            "Connecting to github.com (github.com)|140.82.112.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://objects.githubusercontent.com/github-production-release-asset-2e65be/124585957/47f52b80-3501-11e9-8d2e-dd69a21a4362?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=releaseassetproduction%2F20241201%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20241201T144253Z&X-Amz-Expires=300&X-Amz-Signature=cbc9d5da79da89842761441d6b885461fff6c7cd96f77f7f382b55f9d1887977&X-Amz-SignedHeaders=host&response-content-disposition=attachment%3B%20filename%3DFlickr8k_text.zip&response-content-type=application%2Foctet-stream [following]\n",
            "--2024-12-01 14:42:53--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/124585957/47f52b80-3501-11e9-8d2e-dd69a21a4362?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=releaseassetproduction%2F20241201%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20241201T144253Z&X-Amz-Expires=300&X-Amz-Signature=cbc9d5da79da89842761441d6b885461fff6c7cd96f77f7f382b55f9d1887977&X-Amz-SignedHeaders=host&response-content-disposition=attachment%3B%20filename%3DFlickr8k_text.zip&response-content-type=application%2Foctet-stream\n",
            "Resolving objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2340801 (2.2M) [application/octet-stream]\n",
            "Saving to: ‘Flickr8k_text.zip’\n",
            "\n",
            "Flickr8k_text.zip   100%[===================>]   2.23M  --.-KB/s    in 0.05s   \n",
            "\n",
            "2024-12-01 14:42:53 (45.0 MB/s) - ‘Flickr8k_text.zip’ saved [2340801/2340801]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "import os\n",
        "\n",
        "\n",
        "def unzip_file(zip_file_path, extract_to):\n",
        "    # Create the directory if it doesn't exist\n",
        "    # Path to the ZIP file\n",
        "    zip_file_path = zip_file_path\n",
        "\n",
        "    # Directory where you want to extract the files\n",
        "    extract_to = extract_to\n",
        "\n",
        "    # Create the directory if it doesn't exist\n",
        "    os.makedirs(extract_to, exist_ok=True)\n",
        "\n",
        "    # Open and extract the ZIP file\n",
        "    with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall(extract_to)\n",
        "\n",
        "    print(f\"Files extracted to: {extract_to}\")\n",
        "\n",
        "\n",
        "unzip_file(\"Flickr8k_Dataset.zip\",\"./Flicker8k_Image\")\n",
        "\n",
        "unzip_file(\"Flickr8k_text.zip\",\"./Flicker8k_Text\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UX1gUaQ9fsgT",
        "outputId": "8bcd63b6-745f-4378-abda-9446a7681515"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files extracted to: ./Flicker8k_Image\n",
            "Files extracted to: ./Flicker8k_Text\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# doing it using manual model\n",
        "# doing it using clip model\n"
      ],
      "metadata": {
        "id": "KNACS0YUgB7d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  Approach 2 using clip model\n",
        "#  Create an embedding pair using clip model"
      ],
      "metadata": {
        "id": "G7d5DjZYjTYe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch torchvision transformers ftfy regex\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uhBmQeZdkVtO",
        "outputId": "cdcee87b-3a51-4663-ffa4-7d3f400386a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.20.1+cu121)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.46.2)\n",
            "Collecting ftfy\n",
            "  Downloading ftfy-6.3.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (2024.9.11)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.26.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (11.0.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.26.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.6)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from ftfy) (0.2.13)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n",
            "Downloading ftfy-6.3.1-py3-none-any.whl (44 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.8/44.8 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: ftfy\n",
            "Successfully installed ftfy-6.3.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install git+https://github.com/openai/CLIP.git\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kRoJA_iPmUHb",
        "outputId": "ba6ae9bb-b406-46af-97d6-4fe7367231d7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/openai/CLIP.git\n",
            "  Cloning https://github.com/openai/CLIP.git to /tmp/pip-req-build-iff6pdfe\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/openai/CLIP.git /tmp/pip-req-build-iff6pdfe\n",
            "  Resolved https://github.com/openai/CLIP.git to commit dcba3cb2e2827b402d2701e7e1c7d9fed8a20ef1\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: ftfy in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (6.3.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (24.2)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (2024.9.11)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (4.66.6)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (2.5.1+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (0.20.1+cu121)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from ftfy->clip==1.0) (0.2.13)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch->clip==1.0) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision->clip==1.0) (1.26.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->clip==1.0) (11.0.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->clip==1.0) (3.0.2)\n",
            "Building wheels for collected packages: clip\n",
            "  Building wheel for clip (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for clip: filename=clip-1.0-py3-none-any.whl size=1369489 sha256=1b550ba40617d102af65e51c269fc9e12ecfe5ad22b1b4d0bf79534aae78e7e3\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-8yc89u9j/wheels/da/2b/4c/d6691fa9597aac8bb85d2ac13b112deb897d5b50f5ad9a37e4\n",
            "Successfully built clip\n",
            "Installing collected packages: clip\n",
            "Successfully installed clip-1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import clip\n",
        "from PIL import Image\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "\n",
        "class ImprovedClipDataset(Dataset):\n",
        "    def __init__(self, image_dir, labels_file, model, preprocess, device):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            image_dir (str): Path to the directory containing images.\n",
        "            labels_file (str): Path to the file with labels.\n",
        "            model: CLIP model\n",
        "            preprocess: CLIP preprocessing transform\n",
        "            device: Computation device\n",
        "        \"\"\"\n",
        "        self.image_dir = image_dir\n",
        "        self.model = model\n",
        "        self.preprocess = preprocess\n",
        "        self.device = device\n",
        "\n",
        "        # Parse labels file more robustly\n",
        "        self.image_labels = self._parse_labels_file(labels_file)\n",
        "\n",
        "    def _parse_labels_file(self, labels_file):\n",
        "        \"\"\"\n",
        "        Parse labels file with more robust handling\n",
        "        \"\"\"\n",
        "        image_labels = []\n",
        "        with open(labels_file, 'r', encoding='utf-8') as f:\n",
        "            for line in f:\n",
        "                parts = line.strip().split('\\t')\n",
        "                if len(parts) >= 2:\n",
        "                    img_name = parts[0]\n",
        "                    label = '\\t'.join(parts[1:])  # In case label contains tabs\n",
        "                    image_labels.append((img_name, label))\n",
        "        return image_labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_name, label = self.image_labels[idx]\n",
        "\n",
        "        # Handle image variations (removing #0, #1 etc.)\n",
        "        base_img_name = img_name.split('#')[0]\n",
        "        img_path = os.path.join(self.image_dir, base_img_name)\n",
        "\n",
        "        try:\n",
        "            # Open and preprocess image\n",
        "            image = Image.open(img_path).convert(\"RGB\")\n",
        "            processed_image = self.preprocess(image).unsqueeze(0).to(self.device)\n",
        "\n",
        "            # Tokenize text\n",
        "            text_token = clip.tokenize([label]).to(self.device)\n",
        "\n",
        "            return {\n",
        "                'img_name': img_name,\n",
        "                'processed_image': processed_image,\n",
        "                'text_token': text_token,\n",
        "                'original_label': label\n",
        "            }\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing {img_path}: {e}\")\n",
        "            return None\n",
        "\n",
        "def generate_embeddings(dataloader, model):\n",
        "    \"\"\"\n",
        "    Generate image and text embeddings in batches using DataLoader.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    image_embeddings = []\n",
        "    text_embeddings = []\n",
        "    image_names = []\n",
        "    original_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in dataloader:\n",
        "            if batch is None:\n",
        "                continue\n",
        "\n",
        "            # Generate image embedding\n",
        "            image_emb = model.encode_image(batch['processed_image'].squeeze(1))\n",
        "            image_emb = image_emb / image_emb.norm(dim=-1, keepdim=True)\n",
        "\n",
        "            # Generate text embedding\n",
        "            text_emb = model.encode_text(batch['text_token'].squeeze(1))\n",
        "            text_emb = text_emb / text_emb.norm(dim=-1, keepdim=True)\n",
        "\n",
        "            image_embeddings.append(image_emb.cpu().numpy())\n",
        "            text_embeddings.append(text_emb.cpu().numpy())\n",
        "            image_names.extend(batch['img_name'])\n",
        "            original_labels.extend(batch['original_label'])\n",
        "\n",
        "    return (\n",
        "        torch.cat(image_embeddings).numpy(),\n",
        "        torch.cat(text_embeddings).numpy(),\n",
        "        image_names,\n",
        "        original_labels\n",
        "    )\n",
        "\n",
        "\n",
        "def compute_similarity_matrix(image_embeddings, text_embeddings):\n",
        "    \"\"\"\n",
        "    Compute similarity matrix between image and text embeddings\n",
        "    \"\"\"\n",
        "    similarity_matrix = image_embeddings @ text_embeddings.T\n",
        "    return similarity_matrix\n",
        "\n",
        "# Example usage\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
        "\n",
        "image_path = \"Flicker8k_Image/Flicker8k_Dataset\"\n",
        "text_path = 'Flicker8k_Text/Flickr8k.token.txt'\n",
        "\n",
        "dataset = ImprovedClipDataset(\n",
        "    image_dir=image_path,\n",
        "    labels_file=text_path,\n",
        "    model=model,\n",
        "    preprocess=preprocess,\n",
        "    device=device\n",
        ")\n",
        "\n",
        "dataloader = DataLoader(dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "\n",
        "# Generate embeddings\n",
        "embedded_dataset = generate_embeddings(dataloader, model)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 557
        },
        "id": "S9tJUMIlmKjc",
        "outputId": "68dc706c-6f3a-4443-9526-17954b45ea1d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing Flicker8k_Image/Flicker8k_Dataset/2258277193_586949ec62.jpg.1: [Errno 2] No such file or directory: '/content/Flicker8k_Image/Flicker8k_Dataset/2258277193_586949ec62.jpg.1'\n",
            "Error processing Flicker8k_Image/Flicker8k_Dataset/2258277193_586949ec62.jpg.1: [Errno 2] No such file or directory: '/content/Flicker8k_Image/Flicker8k_Dataset/2258277193_586949ec62.jpg.1'\n",
            "Error processing Flicker8k_Image/Flicker8k_Dataset/2258277193_586949ec62.jpg.1: [Errno 2] No such file or directory: '/content/Flicker8k_Image/Flicker8k_Dataset/2258277193_586949ec62.jpg.1'\n",
            "Error processing Flicker8k_Image/Flicker8k_Dataset/2258277193_586949ec62.jpg.1: [Errno 2] No such file or directory: '/content/Flicker8k_Image/Flicker8k_Dataset/2258277193_586949ec62.jpg.1'\n",
            "Error processing Flicker8k_Image/Flicker8k_Dataset/2258277193_586949ec62.jpg.1: [Errno 2] No such file or directory: '/content/Flicker8k_Image/Flicker8k_Dataset/2258277193_586949ec62.jpg.1'\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "'NoneType' object is not subscriptable",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36mcollate\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    170\u001b[0m                 clone.update(\n\u001b[0;32m--> 171\u001b[0;31m                     {\n\u001b[0m\u001b[1;32m    172\u001b[0m                         key: collate(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    172\u001b[0m                         key: collate(\n\u001b[0;32m--> 173\u001b[0;31m                             \u001b[0;34m[\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    174\u001b[0m                         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    172\u001b[0m                         key: collate(\n\u001b[0;32m--> 173\u001b[0;31m                             \u001b[0;34m[\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    174\u001b[0m                         )\n",
            "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-196e573cdf31>\u001b[0m in \u001b[0;36m<cell line: 130>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;31m# Generate embeddings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m \u001b[0membedded_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-17-196e573cdf31>\u001b[0m in \u001b[0;36mgenerate_embeddings\u001b[0;34m(dataloader, model)\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdataloader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m                 \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    699\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    700\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 701\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    702\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    703\u001b[0m             if (\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    755\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    756\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 757\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    758\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    759\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m    396\u001b[0m         \u001b[0;34m>>\u001b[0m\u001b[0;34m>\u001b[0m \u001b[0mdefault_collate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Handle `CustomType` automatically\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    397\u001b[0m     \"\"\"\n\u001b[0;32m--> 398\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcollate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdefault_collate_fn_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36mcollate\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    189\u001b[0m             \u001b[0;31m# The mapping type may not support `copy()` / `update(mapping)`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m             \u001b[0;31m# or `__init__(iterable)`.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 191\u001b[0;31m             return {\n\u001b[0m\u001b[1;32m    192\u001b[0m                 \u001b[0mkey\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcollate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0melem\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    190\u001b[0m             \u001b[0;31m# or `__init__(iterable)`.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m             return {\n\u001b[0;32m--> 192\u001b[0;31m                 \u001b[0mkey\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcollate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    193\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0melem\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m             }\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    190\u001b[0m             \u001b[0;31m# or `__init__(iterable)`.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m             return {\n\u001b[0;32m--> 192\u001b[0;31m                 \u001b[0mkey\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcollate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    193\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0melem\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m             }\n",
            "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(image_embs), len(text_embs), len(image_names))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "id": "whG9FrWE2pe6",
        "outputId": "bdc7db3d-c628-45ac-c54f-288616a79d2a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "object of type 'NoneType' has no len()",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-9143dff8c97a>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_embs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_embs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m: object of type 'NoneType' has no len()"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "count=0;\n",
        "for data in dataset:\n",
        "  images,images_feature, lables, image_emb, text_emb=data\n",
        "  print(images, lables, \"static data\")\n",
        "  print(image_emb.shape,text_emb.shape, \"shape of embedding\")\n",
        "  similarity = (image_emb @ text_emb.T).item()\n",
        "  print(f\"Similarity Score: {similarity:.4f}\")\n",
        "  count+=1\n",
        "\n",
        "  if count>100:\n",
        "    break;"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L6TmxPxtDVAT",
        "outputId": "084c64dc-878b-4f86-8a61-0dd8d978f83e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Flicker8k_Image/Flicker8k_Dataset/1000268201_693b08cb0e.jpg A child in a pink dress is climbing up a set of stairs in an entry way . static data\n",
            "torch.Size([1, 512]) torch.Size([1, 512]) shape of embedding\n",
            "Similarity Score: 0.3311\n",
            "Flicker8k_Image/Flicker8k_Dataset/1000268201_693b08cb0e.jpg A girl going into a wooden building . static data\n",
            "torch.Size([1, 512]) torch.Size([1, 512]) shape of embedding\n",
            "Similarity Score: 0.2974\n",
            "Flicker8k_Image/Flicker8k_Dataset/1000268201_693b08cb0e.jpg A little girl climbing into a wooden playhouse . static data\n",
            "torch.Size([1, 512]) torch.Size([1, 512]) shape of embedding\n",
            "Similarity Score: 0.3391\n",
            "Flicker8k_Image/Flicker8k_Dataset/1000268201_693b08cb0e.jpg A little girl climbing the stairs to her playhouse . static data\n",
            "torch.Size([1, 512]) torch.Size([1, 512]) shape of embedding\n",
            "Similarity Score: 0.3455\n",
            "Flicker8k_Image/Flicker8k_Dataset/1000268201_693b08cb0e.jpg A little girl in a pink dress going into a wooden cabin . static data\n",
            "torch.Size([1, 512]) torch.Size([1, 512]) shape of embedding\n",
            "Similarity Score: 0.3459\n",
            "Flicker8k_Image/Flicker8k_Dataset/1001773457_577c3a7d70.jpg A black dog and a spotted dog are fighting static data\n",
            "torch.Size([1, 512]) torch.Size([1, 512]) shape of embedding\n",
            "Similarity Score: 0.3142\n",
            "Flicker8k_Image/Flicker8k_Dataset/1001773457_577c3a7d70.jpg A black dog and a tri-colored dog playing with each other on the road . static data\n",
            "torch.Size([1, 512]) torch.Size([1, 512]) shape of embedding\n",
            "Similarity Score: 0.3560\n",
            "Flicker8k_Image/Flicker8k_Dataset/1001773457_577c3a7d70.jpg A black dog and a white dog with brown spots are staring at each other in the street . static data\n",
            "torch.Size([1, 512]) torch.Size([1, 512]) shape of embedding\n",
            "Similarity Score: 0.3362\n",
            "Flicker8k_Image/Flicker8k_Dataset/1001773457_577c3a7d70.jpg Two dogs of different breeds looking at each other on the road . static data\n",
            "torch.Size([1, 512]) torch.Size([1, 512]) shape of embedding\n",
            "Similarity Score: 0.3013\n",
            "Flicker8k_Image/Flicker8k_Dataset/1001773457_577c3a7d70.jpg Two dogs on pavement moving toward each other . static data\n",
            "torch.Size([1, 512]) torch.Size([1, 512]) shape of embedding\n",
            "Similarity Score: 0.3049\n",
            "Flicker8k_Image/Flicker8k_Dataset/1002674143_1b742ab4b8.jpg A little girl covered in paint sits in front of a painted rainbow with her hands in a bowl . static data\n",
            "torch.Size([1, 512]) torch.Size([1, 512]) shape of embedding\n",
            "Similarity Score: 0.3701\n",
            "Flicker8k_Image/Flicker8k_Dataset/1002674143_1b742ab4b8.jpg A little girl is sitting in front of a large painted rainbow . static data\n",
            "torch.Size([1, 512]) torch.Size([1, 512]) shape of embedding\n",
            "Similarity Score: 0.3628\n",
            "Flicker8k_Image/Flicker8k_Dataset/1002674143_1b742ab4b8.jpg A small girl in the grass plays with fingerpaints in front of a white canvas with a rainbow on it . static data\n",
            "torch.Size([1, 512]) torch.Size([1, 512]) shape of embedding\n",
            "Similarity Score: 0.3704\n",
            "Flicker8k_Image/Flicker8k_Dataset/1002674143_1b742ab4b8.jpg There is a girl with pigtails sitting in front of a rainbow painting . static data\n",
            "torch.Size([1, 512]) torch.Size([1, 512]) shape of embedding\n",
            "Similarity Score: 0.3174\n",
            "Flicker8k_Image/Flicker8k_Dataset/1002674143_1b742ab4b8.jpg Young girl with pigtails painting outside in the grass . static data\n",
            "torch.Size([1, 512]) torch.Size([1, 512]) shape of embedding\n",
            "Similarity Score: 0.3169\n",
            "Flicker8k_Image/Flicker8k_Dataset/1003163366_44323f5815.jpg A man lays on a bench while his dog sits by him . static data\n",
            "torch.Size([1, 512]) torch.Size([1, 512]) shape of embedding\n",
            "Similarity Score: 0.3528\n",
            "Flicker8k_Image/Flicker8k_Dataset/1003163366_44323f5815.jpg A man lays on the bench to which a white dog is also tied . static data\n",
            "torch.Size([1, 512]) torch.Size([1, 512]) shape of embedding\n",
            "Similarity Score: 0.3271\n",
            "Flicker8k_Image/Flicker8k_Dataset/1003163366_44323f5815.jpg a man sleeping on a bench outside with a white and black dog sitting next to him . static data\n",
            "torch.Size([1, 512]) torch.Size([1, 512]) shape of embedding\n",
            "Similarity Score: 0.3296\n",
            "Flicker8k_Image/Flicker8k_Dataset/1003163366_44323f5815.jpg A shirtless man lies on a park bench with his dog . static data\n",
            "torch.Size([1, 512]) torch.Size([1, 512]) shape of embedding\n",
            "Similarity Score: 0.3760\n",
            "Flicker8k_Image/Flicker8k_Dataset/1003163366_44323f5815.jpg man laying on bench holding leash of dog sitting on ground static data\n",
            "torch.Size([1, 512]) torch.Size([1, 512]) shape of embedding\n",
            "Similarity Score: 0.3396\n",
            "Flicker8k_Image/Flicker8k_Dataset/1007129816_e794419615.jpg A man in an orange hat starring at something . static data\n",
            "torch.Size([1, 512]) torch.Size([1, 512]) shape of embedding\n",
            "Similarity Score: 0.3113\n",
            "Flicker8k_Image/Flicker8k_Dataset/1007129816_e794419615.jpg A man wears an orange hat and glasses . static data\n",
            "torch.Size([1, 512]) torch.Size([1, 512]) shape of embedding\n",
            "Similarity Score: 0.3318\n",
            "Flicker8k_Image/Flicker8k_Dataset/1007129816_e794419615.jpg A man with gauges and glasses is wearing a Blitz hat . static data\n",
            "torch.Size([1, 512]) torch.Size([1, 512]) shape of embedding\n",
            "Similarity Score: 0.2942\n",
            "Flicker8k_Image/Flicker8k_Dataset/1007129816_e794419615.jpg A man with glasses is wearing a beer can crocheted hat . static data\n",
            "torch.Size([1, 512]) torch.Size([1, 512]) shape of embedding\n",
            "Similarity Score: 0.3574\n",
            "Flicker8k_Image/Flicker8k_Dataset/1007129816_e794419615.jpg The man with pierced ears is wearing glasses and an orange hat . static data\n",
            "torch.Size([1, 512]) torch.Size([1, 512]) shape of embedding\n",
            "Similarity Score: 0.2966\n",
            "Flicker8k_Image/Flicker8k_Dataset/1007320043_627395c3d8.jpg A child playing on a rope net . static data\n",
            "torch.Size([1, 512]) torch.Size([1, 512]) shape of embedding\n",
            "Similarity Score: 0.3223\n",
            "Flicker8k_Image/Flicker8k_Dataset/1007320043_627395c3d8.jpg A little girl climbing on red roping . static data\n",
            "torch.Size([1, 512]) torch.Size([1, 512]) shape of embedding\n",
            "Similarity Score: 0.2949\n",
            "Flicker8k_Image/Flicker8k_Dataset/1007320043_627395c3d8.jpg A little girl in pink climbs a rope bridge at the park . static data\n",
            "torch.Size([1, 512]) torch.Size([1, 512]) shape of embedding\n",
            "Similarity Score: 0.3350\n",
            "Flicker8k_Image/Flicker8k_Dataset/1007320043_627395c3d8.jpg A small child grips onto the red ropes at the playground . static data\n",
            "torch.Size([1, 512]) torch.Size([1, 512]) shape of embedding\n",
            "Similarity Score: 0.3547\n",
            "Flicker8k_Image/Flicker8k_Dataset/1007320043_627395c3d8.jpg The small child climbs on a red ropes on a playground . static data\n",
            "torch.Size([1, 512]) torch.Size([1, 512]) shape of embedding\n",
            "Similarity Score: 0.3528\n",
            "Flicker8k_Image/Flicker8k_Dataset/1009434119_febe49276a.jpg A black and white dog is running in a grassy garden surrounded by a white fence . static data\n",
            "torch.Size([1, 512]) torch.Size([1, 512]) shape of embedding\n",
            "Similarity Score: 0.3267\n",
            "Flicker8k_Image/Flicker8k_Dataset/1009434119_febe49276a.jpg A black and white dog is running through the grass . static data\n",
            "torch.Size([1, 512]) torch.Size([1, 512]) shape of embedding\n",
            "Similarity Score: 0.3140\n",
            "Flicker8k_Image/Flicker8k_Dataset/1009434119_febe49276a.jpg A Boston terrier is running in the grass . static data\n",
            "torch.Size([1, 512]) torch.Size([1, 512]) shape of embedding\n",
            "Similarity Score: 0.3645\n",
            "Flicker8k_Image/Flicker8k_Dataset/1009434119_febe49276a.jpg A Boston Terrier is running on lush green grass in front of a white fence . static data\n",
            "torch.Size([1, 512]) torch.Size([1, 512]) shape of embedding\n",
            "Similarity Score: 0.3523\n",
            "Flicker8k_Image/Flicker8k_Dataset/1009434119_febe49276a.jpg A dog runs on the green grass near a wooden fence . static data\n",
            "torch.Size([1, 512]) torch.Size([1, 512]) shape of embedding\n",
            "Similarity Score: 0.3113\n",
            "Flicker8k_Image/Flicker8k_Dataset/1012212859_01547e3f17.jpg A dog shakes its head near the shore , a red ball next to it . static data\n",
            "torch.Size([1, 512]) torch.Size([1, 512]) shape of embedding\n",
            "Similarity Score: 0.3296\n",
            "Flicker8k_Image/Flicker8k_Dataset/1012212859_01547e3f17.jpg A white dog shakes on the edge of a beach with an orange ball . static data\n",
            "torch.Size([1, 512]) torch.Size([1, 512]) shape of embedding\n",
            "Similarity Score: 0.3359\n",
            "Flicker8k_Image/Flicker8k_Dataset/1012212859_01547e3f17.jpg Dog with orange ball at feet , stands on shore shaking off water static data\n",
            "torch.Size([1, 512]) torch.Size([1, 512]) shape of embedding\n",
            "Similarity Score: 0.3098\n",
            "Flicker8k_Image/Flicker8k_Dataset/1012212859_01547e3f17.jpg White dog playing with a red ball on the shore near the water . static data\n",
            "torch.Size([1, 512]) torch.Size([1, 512]) shape of embedding\n",
            "Similarity Score: 0.3101\n",
            "Flicker8k_Image/Flicker8k_Dataset/1012212859_01547e3f17.jpg White dog with brown ears standing near water with head turned to one side . static data\n",
            "torch.Size([1, 512]) torch.Size([1, 512]) shape of embedding\n",
            "Similarity Score: 0.2886\n",
            "Flicker8k_Image/Flicker8k_Dataset/1015118661_980735411b.jpg A boy smiles in front of a stony wall in a city . static data\n",
            "torch.Size([1, 512]) torch.Size([1, 512]) shape of embedding\n",
            "Similarity Score: 0.2961\n",
            "Flicker8k_Image/Flicker8k_Dataset/1015118661_980735411b.jpg A little boy is standing on the street while a man in overalls is working on a stone wall . static data\n",
            "torch.Size([1, 512]) torch.Size([1, 512]) shape of embedding\n",
            "Similarity Score: 0.3391\n",
            "Flicker8k_Image/Flicker8k_Dataset/1015118661_980735411b.jpg A young boy runs aross the street . static data\n",
            "torch.Size([1, 512]) torch.Size([1, 512]) shape of embedding\n",
            "Similarity Score: 0.3162\n",
            "Flicker8k_Image/Flicker8k_Dataset/1015118661_980735411b.jpg A young child is walking on a stone paved street with a metal pole and a man behind him . static data\n",
            "torch.Size([1, 512]) torch.Size([1, 512]) shape of embedding\n",
            "Similarity Score: 0.3547\n",
            "Flicker8k_Image/Flicker8k_Dataset/1015118661_980735411b.jpg Smiling boy in white shirt and blue jeans in front of rock wall with man in overalls behind him . static data\n",
            "torch.Size([1, 512]) torch.Size([1, 512]) shape of embedding\n",
            "Similarity Score: 0.3567\n",
            "Flicker8k_Image/Flicker8k_Dataset/1015584366_dfcec3c85a.jpg A black dog leaps over a log . static data\n",
            "torch.Size([1, 512]) torch.Size([1, 512]) shape of embedding\n",
            "Similarity Score: 0.3604\n",
            "Flicker8k_Image/Flicker8k_Dataset/1015584366_dfcec3c85a.jpg A grey dog is leaping over a fallen tree . static data\n",
            "torch.Size([1, 512]) torch.Size([1, 512]) shape of embedding\n",
            "Similarity Score: 0.3433\n",
            "Flicker8k_Image/Flicker8k_Dataset/1015584366_dfcec3c85a.jpg A large black dog leaps a fallen log . static data\n",
            "torch.Size([1, 512]) torch.Size([1, 512]) shape of embedding\n",
            "Similarity Score: 0.3503\n",
            "Flicker8k_Image/Flicker8k_Dataset/1015584366_dfcec3c85a.jpg A mottled black and grey dog in a blue collar jumping over a fallen tree . static data\n",
            "torch.Size([1, 512]) torch.Size([1, 512]) shape of embedding\n",
            "Similarity Score: 0.3665\n",
            "Flicker8k_Image/Flicker8k_Dataset/1015584366_dfcec3c85a.jpg The black dog jumped the tree stump . static data\n",
            "torch.Size([1, 512]) torch.Size([1, 512]) shape of embedding\n",
            "Similarity Score: 0.3162\n",
            "Flicker8k_Image/Flicker8k_Dataset/101654506_8eb26cfb60.jpg A brown and white dog is running through the snow . static data\n",
            "torch.Size([1, 512]) torch.Size([1, 512]) shape of embedding\n",
            "Similarity Score: 0.3628\n",
            "Flicker8k_Image/Flicker8k_Dataset/101654506_8eb26cfb60.jpg A dog is running in the snow static data\n",
            "torch.Size([1, 512]) torch.Size([1, 512]) shape of embedding\n",
            "Similarity Score: 0.3342\n",
            "Flicker8k_Image/Flicker8k_Dataset/101654506_8eb26cfb60.jpg A dog running through snow . static data\n",
            "torch.Size([1, 512]) torch.Size([1, 512]) shape of embedding\n",
            "Similarity Score: 0.3384\n",
            "Flicker8k_Image/Flicker8k_Dataset/101654506_8eb26cfb60.jpg a white and brown dog is running through a snow covered field . static data\n",
            "torch.Size([1, 512]) torch.Size([1, 512]) shape of embedding\n",
            "Similarity Score: 0.3704\n",
            "Flicker8k_Image/Flicker8k_Dataset/101654506_8eb26cfb60.jpg The white and brown dog is running over the surface of the snow . static data\n",
            "torch.Size([1, 512]) torch.Size([1, 512]) shape of embedding\n",
            "Similarity Score: 0.3235\n",
            "Flicker8k_Image/Flicker8k_Dataset/101669240_b2d3e7f17b.jpg A man in a hat is displaying pictures next to a skier in a blue hat . static data\n",
            "torch.Size([1, 512]) torch.Size([1, 512]) shape of embedding\n",
            "Similarity Score: 0.3347\n",
            "Flicker8k_Image/Flicker8k_Dataset/101669240_b2d3e7f17b.jpg A man skis past another man displaying paintings in the snow . static data\n",
            "torch.Size([1, 512]) torch.Size([1, 512]) shape of embedding\n",
            "Similarity Score: 0.3579\n",
            "Flicker8k_Image/Flicker8k_Dataset/101669240_b2d3e7f17b.jpg A person wearing skis looking at framed pictures set up in the snow . static data\n",
            "torch.Size([1, 512]) torch.Size([1, 512]) shape of embedding\n",
            "Similarity Score: 0.3376\n",
            "Flicker8k_Image/Flicker8k_Dataset/101669240_b2d3e7f17b.jpg A skier looks at framed pictures in the snow next to trees . static data\n",
            "torch.Size([1, 512]) torch.Size([1, 512]) shape of embedding\n",
            "Similarity Score: 0.3267\n",
            "Flicker8k_Image/Flicker8k_Dataset/101669240_b2d3e7f17b.jpg Man on skis looking at artwork for sale in the snow static data\n",
            "torch.Size([1, 512]) torch.Size([1, 512]) shape of embedding\n",
            "Similarity Score: 0.3269\n",
            "Flicker8k_Image/Flicker8k_Dataset/1016887272_03199f49c4.jpg A collage of one person climbing a cliff . static data\n",
            "torch.Size([1, 512]) torch.Size([1, 512]) shape of embedding\n",
            "Similarity Score: 0.2744\n",
            "Flicker8k_Image/Flicker8k_Dataset/1016887272_03199f49c4.jpg A group of people are rock climbing on a rock climbing wall . static data\n",
            "torch.Size([1, 512]) torch.Size([1, 512]) shape of embedding\n",
            "Similarity Score: 0.3516\n",
            "Flicker8k_Image/Flicker8k_Dataset/1016887272_03199f49c4.jpg A group of people climbing a rock while one man belays static data\n",
            "torch.Size([1, 512]) torch.Size([1, 512]) shape of embedding\n",
            "Similarity Score: 0.3521\n",
            "Flicker8k_Image/Flicker8k_Dataset/1016887272_03199f49c4.jpg Seven climbers are ascending a rock face whilst another man stands holding the rope . static data\n",
            "torch.Size([1, 512]) torch.Size([1, 512]) shape of embedding\n",
            "Similarity Score: 0.3757\n",
            "Flicker8k_Image/Flicker8k_Dataset/1016887272_03199f49c4.jpg Several climbers in a row are climbing the rock while the man in red watches and holds the line . static data\n",
            "torch.Size([1, 512]) torch.Size([1, 512]) shape of embedding\n",
            "Similarity Score: 0.3833\n",
            "Flicker8k_Image/Flicker8k_Dataset/1019077836_6fc9b15408.jpg A brown dog chases the water from a sprinkler on a lawn . static data\n",
            "torch.Size([1, 512]) torch.Size([1, 512]) shape of embedding\n",
            "Similarity Score: 0.4043\n",
            "Flicker8k_Image/Flicker8k_Dataset/1019077836_6fc9b15408.jpg a brown dog plays with the hose . static data\n",
            "torch.Size([1, 512]) torch.Size([1, 512]) shape of embedding\n",
            "Similarity Score: 0.3860\n",
            "Flicker8k_Image/Flicker8k_Dataset/1019077836_6fc9b15408.jpg A brown dog running on a lawn near a garden hose static data\n",
            "torch.Size([1, 512]) torch.Size([1, 512]) shape of embedding\n",
            "Similarity Score: 0.4180\n",
            "Flicker8k_Image/Flicker8k_Dataset/1019077836_6fc9b15408.jpg A dog is playing with a hose . static data\n",
            "torch.Size([1, 512]) torch.Size([1, 512]) shape of embedding\n",
            "Similarity Score: 0.3584\n",
            "Flicker8k_Image/Flicker8k_Dataset/1019077836_6fc9b15408.jpg Large brown dog running away from the sprinkler in the grass . static data\n",
            "torch.Size([1, 512]) torch.Size([1, 512]) shape of embedding\n",
            "Similarity Score: 0.3545\n",
            "Flicker8k_Image/Flicker8k_Dataset/1019604187_d087bf9a5f.jpg A dog prepares to catch a thrown object in a field with nearby cars . static data\n",
            "torch.Size([1, 512]) torch.Size([1, 512]) shape of embedding\n",
            "Similarity Score: 0.3245\n",
            "Flicker8k_Image/Flicker8k_Dataset/1019604187_d087bf9a5f.jpg A white dog is about to catch a yellow ball in its mouth . static data\n",
            "torch.Size([1, 512]) torch.Size([1, 512]) shape of embedding\n",
            "Similarity Score: 0.3560\n",
            "Flicker8k_Image/Flicker8k_Dataset/1019604187_d087bf9a5f.jpg A white dog is about to catch a yellow dog toy . static data\n",
            "torch.Size([1, 512]) torch.Size([1, 512]) shape of embedding\n",
            "Similarity Score: 0.3188\n",
            "Flicker8k_Image/Flicker8k_Dataset/1019604187_d087bf9a5f.jpg A white dog is ready to catch a yellow ball flying through the air . static data\n",
            "torch.Size([1, 512]) torch.Size([1, 512]) shape of embedding\n",
            "Similarity Score: 0.3677\n",
            "Flicker8k_Image/Flicker8k_Dataset/1019604187_d087bf9a5f.jpg A white dog running after a yellow ball static data\n",
            "torch.Size([1, 512]) torch.Size([1, 512]) shape of embedding\n",
            "Similarity Score: 0.3608\n",
            "Flicker8k_Image/Flicker8k_Dataset/1020651753_06077ec457.jpg a black and white dog jumping in the air to get a toy . static data\n",
            "torch.Size([1, 512]) torch.Size([1, 512]) shape of embedding\n",
            "Similarity Score: 0.3445\n",
            "Flicker8k_Image/Flicker8k_Dataset/1020651753_06077ec457.jpg A black and white dog jumps up towards a yellow toy . static data\n",
            "torch.Size([1, 512]) torch.Size([1, 512]) shape of embedding\n",
            "Similarity Score: 0.3301\n",
            "Flicker8k_Image/Flicker8k_Dataset/1020651753_06077ec457.jpg A dog leaps to catch a ball in a field . static data\n",
            "torch.Size([1, 512]) torch.Size([1, 512]) shape of embedding\n",
            "Similarity Score: 0.3396\n",
            "Flicker8k_Image/Flicker8k_Dataset/1020651753_06077ec457.jpg A white dog is trying to catch a ball in midair over a grassy field . static data\n",
            "torch.Size([1, 512]) torch.Size([1, 512]) shape of embedding\n",
            "Similarity Score: 0.3652\n",
            "Flicker8k_Image/Flicker8k_Dataset/1020651753_06077ec457.jpg The white dog is playing in a green field with a yellow toy . static data\n",
            "torch.Size([1, 512]) torch.Size([1, 512]) shape of embedding\n",
            "Similarity Score: 0.3098\n",
            "Flicker8k_Image/Flicker8k_Dataset/1022454332_6af2c1449a.jpg A child and a woman are at waters edge in a big city . static data\n",
            "torch.Size([1, 512]) torch.Size([1, 512]) shape of embedding\n",
            "Similarity Score: 0.3093\n",
            "Flicker8k_Image/Flicker8k_Dataset/1022454332_6af2c1449a.jpg a large lake with a lone duck swimming in it with several people around the edge of it . static data\n",
            "torch.Size([1, 512]) torch.Size([1, 512]) shape of embedding\n",
            "Similarity Score: 0.2827\n",
            "Flicker8k_Image/Flicker8k_Dataset/1022454332_6af2c1449a.jpg A little boy at a lake watching a duck . static data\n",
            "torch.Size([1, 512]) torch.Size([1, 512]) shape of embedding\n",
            "Similarity Score: 0.2981\n",
            "Flicker8k_Image/Flicker8k_Dataset/1022454332_6af2c1449a.jpg A young boy waves his hand at the duck in the water surrounded by a green park . static data\n",
            "torch.Size([1, 512]) torch.Size([1, 512]) shape of embedding\n",
            "Similarity Score: 0.3689\n",
            "Flicker8k_Image/Flicker8k_Dataset/1022454332_6af2c1449a.jpg Two people are at the edge of a lake , facing the water and the city skyline . static data\n",
            "torch.Size([1, 512]) torch.Size([1, 512]) shape of embedding\n",
            "Similarity Score: 0.3262\n",
            "Flicker8k_Image/Flicker8k_Dataset/1022454428_b6b660a67b.jpg A couple and an infant , being held by the male , sitting next to a pond with a near by stroller . static data\n",
            "torch.Size([1, 512]) torch.Size([1, 512]) shape of embedding\n",
            "Similarity Score: 0.3162\n",
            "Flicker8k_Image/Flicker8k_Dataset/1022454428_b6b660a67b.jpg A couple sit on the grass with a baby and stroller . static data\n",
            "torch.Size([1, 512]) torch.Size([1, 512]) shape of embedding\n",
            "Similarity Score: 0.3208\n",
            "Flicker8k_Image/Flicker8k_Dataset/1022454428_b6b660a67b.jpg A couple with their newborn baby sitting under a tree facing a lake . static data\n",
            "torch.Size([1, 512]) torch.Size([1, 512]) shape of embedding\n",
            "Similarity Score: 0.2966\n",
            "Flicker8k_Image/Flicker8k_Dataset/1022454428_b6b660a67b.jpg A man and woman care for an infant along the side of a body of water . static data\n",
            "torch.Size([1, 512]) torch.Size([1, 512]) shape of embedding\n",
            "Similarity Score: 0.2678\n",
            "Flicker8k_Image/Flicker8k_Dataset/1022454428_b6b660a67b.jpg Couple with a baby sit outdoors next to their stroller . static data\n",
            "torch.Size([1, 512]) torch.Size([1, 512]) shape of embedding\n",
            "Similarity Score: 0.3069\n",
            "Flicker8k_Image/Flicker8k_Dataset/1022975728_75515238d8.jpg A black dog running in the surf . static data\n",
            "torch.Size([1, 512]) torch.Size([1, 512]) shape of embedding\n",
            "Similarity Score: 0.3394\n",
            "Flicker8k_Image/Flicker8k_Dataset/1022975728_75515238d8.jpg A black lab with tags frolicks in the water . static data\n",
            "torch.Size([1, 512]) torch.Size([1, 512]) shape of embedding\n",
            "Similarity Score: 0.3425\n",
            "Flicker8k_Image/Flicker8k_Dataset/1022975728_75515238d8.jpg A dog splashes in the water static data\n",
            "torch.Size([1, 512]) torch.Size([1, 512]) shape of embedding\n",
            "Similarity Score: 0.2954\n",
            "Flicker8k_Image/Flicker8k_Dataset/1022975728_75515238d8.jpg The black dog runs through the water . static data\n",
            "torch.Size([1, 512]) torch.Size([1, 512]) shape of embedding\n",
            "Similarity Score: 0.3264\n",
            "Flicker8k_Image/Flicker8k_Dataset/1022975728_75515238d8.jpg This is a black dog splashing in the water . static data\n",
            "torch.Size([1, 512]) torch.Size([1, 512]) shape of embedding\n",
            "Similarity Score: 0.3271\n",
            "Flicker8k_Image/Flicker8k_Dataset/102351840_323e3de834.jpg A man drilling a hole in the ice . static data\n",
            "torch.Size([1, 512]) torch.Size([1, 512]) shape of embedding\n",
            "Similarity Score: 0.3354\n",
            "Flicker8k_Image/Flicker8k_Dataset/102351840_323e3de834.jpg A man is drilling through the frozen ice of a pond . static data\n",
            "torch.Size([1, 512]) torch.Size([1, 512]) shape of embedding\n",
            "Similarity Score: 0.3198\n",
            "Flicker8k_Image/Flicker8k_Dataset/102351840_323e3de834.jpg A person in the snow drilling a hole in the ice . static data\n",
            "torch.Size([1, 512]) torch.Size([1, 512]) shape of embedding\n",
            "Similarity Score: 0.3408\n",
            "Flicker8k_Image/Flicker8k_Dataset/102351840_323e3de834.jpg A person standing on a frozen lake . static data\n",
            "torch.Size([1, 512]) torch.Size([1, 512]) shape of embedding\n",
            "Similarity Score: 0.2949\n",
            "Flicker8k_Image/Flicker8k_Dataset/102351840_323e3de834.jpg Two men are ice fishing . static data\n",
            "torch.Size([1, 512]) torch.Size([1, 512]) shape of embedding\n",
            "Similarity Score: 0.3362\n",
            "Flicker8k_Image/Flicker8k_Dataset/1024138940_f1fefbdce1.jpg Two different breeds of brown and white dogs play on the beach . static data\n",
            "torch.Size([1, 512]) torch.Size([1, 512]) shape of embedding\n",
            "Similarity Score: 0.3411\n"
          ]
        }
      ]
    }
  ]
}