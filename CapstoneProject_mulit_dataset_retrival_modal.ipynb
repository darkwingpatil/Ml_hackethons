{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/darkwingpatil/Ml_hackethons/blob/main/CapstoneProject_mulit_dataset_retrival_modal.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Multi media retrival  "
      ],
      "metadata": {
        "id": "wlQwGe9EcF54"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# For image download\n",
        "!wget https://github.com/jbrownlee/Datasets/releases/download/Flickr8k/Flickr8k_Dataset.zip\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ui0WsV6rcdTg",
        "outputId": "d2b48495-c5cf-4422-ef91-f9a71d09a0fe"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-12-08 05:40:14--  https://github.com/jbrownlee/Datasets/releases/download/Flickr8k/Flickr8k_Dataset.zip\n",
            "Resolving github.com (github.com)... 140.82.114.3\n",
            "Connecting to github.com (github.com)|140.82.114.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://objects.githubusercontent.com/github-production-release-asset-2e65be/124585957/47f52b80-3501-11e9-8f49-4515a2a3339b?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=releaseassetproduction%2F20241208%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20241208T054014Z&X-Amz-Expires=300&X-Amz-Signature=cb0a899336a2630088628be12de111efaeabfdc1528876e82f95e404b06ef012&X-Amz-SignedHeaders=host&response-content-disposition=attachment%3B%20filename%3DFlickr8k_Dataset.zip&response-content-type=application%2Foctet-stream [following]\n",
            "--2024-12-08 05:40:14--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/124585957/47f52b80-3501-11e9-8f49-4515a2a3339b?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=releaseassetproduction%2F20241208%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20241208T054014Z&X-Amz-Expires=300&X-Amz-Signature=cb0a899336a2630088628be12de111efaeabfdc1528876e82f95e404b06ef012&X-Amz-SignedHeaders=host&response-content-disposition=attachment%3B%20filename%3DFlickr8k_Dataset.zip&response-content-type=application%2Foctet-stream\n",
            "Resolving objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115419746 (1.0G) [application/octet-stream]\n",
            "Saving to: ‘Flickr8k_Dataset.zip’\n",
            "\n",
            "Flickr8k_Dataset.zi 100%[===================>]   1.04G  94.0MB/s    in 12s     \n",
            "\n",
            "2024-12-08 05:40:26 (90.4 MB/s) - ‘Flickr8k_Dataset.zip’ saved [1115419746/1115419746]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# for caption dataset downlaod\n",
        "!wget https://github.com/jbrownlee/Datasets/releases/download/Flickr8k/Flickr8k_text.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E8EboKJ8g5K2",
        "outputId": "87528ac1-f181-4954-a065-93548f5efab4"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-12-08 05:40:48--  https://github.com/jbrownlee/Datasets/releases/download/Flickr8k/Flickr8k_text.zip\n",
            "Resolving github.com (github.com)... 140.82.114.4\n",
            "Connecting to github.com (github.com)|140.82.114.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://objects.githubusercontent.com/github-production-release-asset-2e65be/124585957/47f52b80-3501-11e9-8d2e-dd69a21a4362?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=releaseassetproduction%2F20241208%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20241208T054048Z&X-Amz-Expires=300&X-Amz-Signature=39dd7a1801c5ac0d8490b2eeb5f58dec70f8da023f216c7bf5bfb646bd0141bc&X-Amz-SignedHeaders=host&response-content-disposition=attachment%3B%20filename%3DFlickr8k_text.zip&response-content-type=application%2Foctet-stream [following]\n",
            "--2024-12-08 05:40:48--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/124585957/47f52b80-3501-11e9-8d2e-dd69a21a4362?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=releaseassetproduction%2F20241208%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20241208T054048Z&X-Amz-Expires=300&X-Amz-Signature=39dd7a1801c5ac0d8490b2eeb5f58dec70f8da023f216c7bf5bfb646bd0141bc&X-Amz-SignedHeaders=host&response-content-disposition=attachment%3B%20filename%3DFlickr8k_text.zip&response-content-type=application%2Foctet-stream\n",
            "Resolving objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2340801 (2.2M) [application/octet-stream]\n",
            "Saving to: ‘Flickr8k_text.zip’\n",
            "\n",
            "Flickr8k_text.zip   100%[===================>]   2.23M  --.-KB/s    in 0.04s   \n",
            "\n",
            "2024-12-08 05:40:48 (58.5 MB/s) - ‘Flickr8k_text.zip’ saved [2340801/2340801]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "import os\n",
        "\n",
        "\n",
        "def unzip_file(zip_file_path, extract_to):\n",
        "    # Create the directory if it doesn't exist\n",
        "    # Path to the ZIP file\n",
        "    zip_file_path = zip_file_path\n",
        "\n",
        "    # Directory where you want to extract the files\n",
        "    extract_to = extract_to\n",
        "\n",
        "    # Create the directory if it doesn't exist\n",
        "    os.makedirs(extract_to, exist_ok=True)\n",
        "\n",
        "    # Open and extract the ZIP file\n",
        "    with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall(extract_to)\n",
        "\n",
        "    print(f\"Files extracted to: {extract_to}\")\n",
        "\n",
        "\n",
        "unzip_file(\"Flickr8k_Dataset.zip\",\"./Flicker8k_Image\")\n",
        "\n",
        "unzip_file(\"Flickr8k_text.zip\",\"./Flicker8k_Text\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UX1gUaQ9fsgT",
        "outputId": "0add437a-c4f5-469c-8c5e-0faf0a80b64b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files extracted to: ./Flicker8k_Image\n",
            "Files extracted to: ./Flicker8k_Text\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# doing it using manual model\n",
        "# doing it using clip model\n"
      ],
      "metadata": {
        "id": "KNACS0YUgB7d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  Approach 2 using clip model\n",
        "#  Create an embedding pair using clip model"
      ],
      "metadata": {
        "id": "G7d5DjZYjTYe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch torchvision transformers ftfy regex\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uhBmQeZdkVtO",
        "outputId": "0f4a2c50-365d-4b20-c928-041447122475",
        "collapsed": true
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.20.1+cu121)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.46.3)\n",
            "Collecting ftfy\n",
            "  Downloading ftfy-6.3.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (2024.9.11)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.26.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (11.0.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.26.3)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.6)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from ftfy) (0.2.13)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n",
            "Downloading ftfy-6.3.1-py3-none-any.whl (44 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.8/44.8 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: ftfy\n",
            "Successfully installed ftfy-6.3.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install git+https://github.com/openai/CLIP.git\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kRoJA_iPmUHb",
        "outputId": "3ec4e4f5-50f4-442d-984a-f2e27c0ce2cf",
        "collapsed": true
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/openai/CLIP.git\n",
            "  Cloning https://github.com/openai/CLIP.git to /tmp/pip-req-build-rts10glf\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/openai/CLIP.git /tmp/pip-req-build-rts10glf\n",
            "  Resolved https://github.com/openai/CLIP.git to commit dcba3cb2e2827b402d2701e7e1c7d9fed8a20ef1\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: ftfy in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (6.3.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (24.2)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (2024.9.11)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (4.66.6)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (2.5.1+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (0.20.1+cu121)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from ftfy->clip==1.0) (0.2.13)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch->clip==1.0) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision->clip==1.0) (1.26.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->clip==1.0) (11.0.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->clip==1.0) (3.0.2)\n",
            "Building wheels for collected packages: clip\n",
            "  Building wheel for clip (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for clip: filename=clip-1.0-py3-none-any.whl size=1369489 sha256=4ef7f47b5adccb44587dfdedf42b01e840bb091ddf45499e2d8e556114d3e676\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-l9ri1yxr/wheels/da/2b/4c/d6691fa9597aac8bb85d2ac13b112deb897d5b50f5ad9a37e4\n",
            "Successfully built clip\n",
            "Installing collected packages: clip\n",
            "Successfully installed clip-1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import clip\n",
        "from PIL import Image\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "\n",
        "class ImprovedClipDataset(Dataset):\n",
        "    def __init__(self, image_dir, labels_file, model, preprocess, device):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            image_dir (str): Path to the directory containing images.\n",
        "            labels_file (str): Path to the file with labels.\n",
        "            model: CLIP model\n",
        "            preprocess: CLIP preprocessing transform\n",
        "            device: Computation device\n",
        "        \"\"\"\n",
        "        self.image_dir = image_dir\n",
        "        self.model = model\n",
        "        self.preprocess = preprocess\n",
        "        self.device = device\n",
        "\n",
        "        # Parse labels file more robustly\n",
        "        self.image_labels = self._parse_labels_file(labels_file)\n",
        "\n",
        "    def _parse_labels_file(self, labels_file):\n",
        "        \"\"\"\n",
        "        Parse labels file with more robust handling\n",
        "        \"\"\"\n",
        "        image_labels = []\n",
        "        with open(labels_file, 'r', encoding='utf-8') as f:\n",
        "            for line in f:\n",
        "                parts = line.strip().split('\\t')\n",
        "                if len(parts) >= 2:\n",
        "                    img_name = parts[0].split('#')[0]\n",
        "                    label = '\\t'.join(parts[1:])  # In case label contains tabs\n",
        "                    image_labels.append((img_name, label))\n",
        "        return image_labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_name, label = self.image_labels[idx]\n",
        "\n",
        "        # Handle image variations (removing #0, #1 etc.)\n",
        "        base_img_name = img_name\n",
        "        img_path = os.path.join(self.image_dir, base_img_name)\n",
        "\n",
        "        try:\n",
        "            # Open and preprocess image\n",
        "            image = Image.open(img_path).convert(\"RGB\")\n",
        "            processed_image = self.preprocess(image).unsqueeze(0).to(self.device)\n",
        "\n",
        "            # Tokenize text\n",
        "            text_token = clip.tokenize([label]).to(self.device)\n",
        "\n",
        "            return {\n",
        "                'img_name': img_name,\n",
        "                'processed_image': processed_image,\n",
        "                'text_token': text_token,\n",
        "                'original_label': label\n",
        "            }\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing {img_path}: {e}\")\n",
        "            return {\n",
        "                'img_name': img_name,\n",
        "                'processed_image': torch.zeros(1, 3, 224, 224, device=self.device), # Dummy image tensor\n",
        "                'text_token': clip.tokenize([\"\"]).to(self.device), # Dummy text token\n",
        "                'original_label': label\n",
        "            }\n",
        "\n",
        "def generate_embeddings(dataloader, model):\n",
        "    \"\"\"\n",
        "    Generate image and text embeddings in batches using DataLoader.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    image_embeddings = []\n",
        "    text_embeddings = []\n",
        "    image_names = []\n",
        "    original_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in dataloader:\n",
        "            if batch is None:\n",
        "                continue\n",
        "\n",
        "            processed_images = batch['processed_image'].squeeze(1)\n",
        "            labels = batch['original_label']\n",
        "            text_tokens = clip.tokenize(labels, truncate=True).to(device)\n",
        "\n",
        "            # Generate embeddings\n",
        "            image_emb = model.encode_image(processed_images)\n",
        "            text_emb = model.encode_text(text_tokens)\n",
        "\n",
        "            # Normalize embeddings\n",
        "            image_emb = image_emb / image_emb.norm(dim=-1, keepdim=True)\n",
        "            text_emb = text_emb / text_emb.norm(dim=-1, keepdim=True)\n",
        "\n",
        "            image_embeddings.append(image_emb)\n",
        "            text_embeddings.append(text_emb)\n",
        "            image_names.extend(batch['img_name'])\n",
        "            original_labels.extend(batch['original_label'])\n",
        "    return (\n",
        "        torch.cat(image_embeddings),\n",
        "        torch.cat(text_embeddings),\n",
        "        image_names,\n",
        "        original_labels\n",
        "    )\n",
        "\n",
        "\n",
        "def compute_similarity_matrix(image_embeddings, text_embeddings):\n",
        "    \"\"\"\n",
        "    Compute similarity matrix between image and text embeddings\n",
        "    \"\"\"\n",
        "    similarity_matrix = image_embeddings @ text_embeddings.T\n",
        "    return similarity_matrix\n",
        "\n",
        "# Example usage\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
        "\n",
        "image_path = \"Flicker8k_Image/Flicker8k_Dataset\"\n",
        "text_path = 'Flicker8k_Text/Flickr8k.token.txt'\n",
        "\n",
        "dataset = ImprovedClipDataset(\n",
        "    image_dir=image_path,\n",
        "    labels_file=text_path,\n",
        "    model=model,\n",
        "    preprocess=preprocess,\n",
        "    device=device\n",
        ")\n",
        "\n",
        "dataloader = DataLoader(dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "\n",
        "# Generate embeddings\n",
        "embedded_dataset = generate_embeddings(dataloader, model)\n"
      ],
      "metadata": {
        "id": "S9tJUMIlmKjc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5eb8cb5f-128c-427f-8a13-1d3aeb85797e"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|███████████████████████████████████████| 338M/338M [00:06<00:00, 56.9MiB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing Flicker8k_Image/Flicker8k_Dataset/2258277193_586949ec62.jpg.1: [Errno 2] No such file or directory: '/content/Flicker8k_Image/Flicker8k_Dataset/2258277193_586949ec62.jpg.1'\n",
            "Error processing Flicker8k_Image/Flicker8k_Dataset/2258277193_586949ec62.jpg.1: [Errno 2] No such file or directory: '/content/Flicker8k_Image/Flicker8k_Dataset/2258277193_586949ec62.jpg.1'\n",
            "Error processing Flicker8k_Image/Flicker8k_Dataset/2258277193_586949ec62.jpg.1: [Errno 2] No such file or directory: '/content/Flicker8k_Image/Flicker8k_Dataset/2258277193_586949ec62.jpg.1'\n",
            "Error processing Flicker8k_Image/Flicker8k_Dataset/2258277193_586949ec62.jpg.1: [Errno 2] No such file or directory: '/content/Flicker8k_Image/Flicker8k_Dataset/2258277193_586949ec62.jpg.1'\n",
            "Error processing Flicker8k_Image/Flicker8k_Dataset/2258277193_586949ec62.jpg.1: [Errno 2] No such file or directory: '/content/Flicker8k_Image/Flicker8k_Dataset/2258277193_586949ec62.jpg.1'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(embedded_dataset[0]), len(embedded_dataset[1]), len(embedded_dataset[2]), len(embedded_dataset[3]))"
      ],
      "metadata": {
        "id": "whG9FrWE2pe6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d7736300-712e-4362-b967-2c36c8adb329"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "40460 40460 40460 40460\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "# Move embeddings to GPU\n",
        "img_embs = embedded_dataset[0].to('cuda')  # Image embeddings (N x D)\n",
        "txt_embs = embedded_dataset[1].to('cuda')  # Text embeddings (M x D)\n",
        "images = embedded_dataset[2]               # List of images (indices or paths)\n",
        "\n",
        "# Compute similarities in batch: (N x D) @ (D x M) -> (N x M)\n",
        "similarities = torch.matmul(img_embs, txt_embs.T)  # Shape: (N, M)\n",
        "\n",
        "# Get the indices of the highest similarity text for each image\n",
        "max_similarity_indices = torch.argmax(similarities, dim=1)  # Shape: (N,)\n",
        "\n",
        "# Compare actual and retrieved images\n",
        "all_list_emb = []\n",
        "\n",
        "for index, max_idx in enumerate(max_similarity_indices):\n",
        "    actual_image = images[index]\n",
        "    retrieved_image = images[max_idx]\n",
        "\n",
        "    if actual_image == retrieved_image:\n",
        "        all_list_emb.append(1)\n",
        "    else:\n",
        "        all_list_emb.append(0)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = np.mean(all_list_emb) * 100\n",
        "print(f\"Accuracy = {accuracy:.2f}%\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "07PqDUfMUOIT",
        "outputId": "5d7774d1-dac8-4a0c-a19f-9d287acf481b"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy = 47.31%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "# Ensure computation runs on GPU if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Assuming embedded_dataset is a tuple of (image_embeddings, text_embeddings, image_names)\n",
        "image_embeddings, text_embeddings, image_names,_ = embedded_dataset\n",
        "image_embeddings = image_embeddings.to(device)\n",
        "text_embeddings = text_embeddings.to(device)\n",
        "\n",
        "# Normalize embeddings for cosine similarity\n",
        "image_embeddings = image_embeddings / image_embeddings.norm(dim=-1, keepdim=True)\n",
        "text_embeddings = text_embeddings / text_embeddings.norm(dim=-1, keepdim=True)\n",
        "\n",
        "# Compute the similarity matrix in one batch operation\n",
        "similarity_matrix = image_embeddings @ text_embeddings.T  # Shape: (num_images, num_texts)\n",
        "\n",
        "# Find the index of the best-matching text for each image\n",
        "max_similarity_indices = similarity_matrix.argmax(dim=1)\n",
        "\n",
        "# Compare the retrieved images with the actual images\n",
        "all_list_emb = [\n",
        "    1 if image_names[img_idx] == image_names[retrieved_idx] else 0\n",
        "    for img_idx, retrieved_idx in enumerate(max_similarity_indices)\n",
        "]\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = np.mean(all_list_emb) * 100\n",
        "print(f\"Accuracy = {accuracy:.2f}%\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kNawDIVgVP4U",
        "outputId": "425f3c30-7f48-4628-8e37-e4fd567f2137"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy = 47.36%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "count=0;\n",
        "image_embeddings, text_embeddings, image_names, original_labels = embedded_dataset\n",
        "\n",
        "for i in range(len(image_embeddings)):\n",
        "    print(image_names[i], original_labels[i], \"static data\")\n",
        "    similarity = compute_similarity_matrix(image_embeddings[i],text_embeddings[i])\n",
        "    print(f\"Similarity Score: {similarity:.4f}\")\n",
        "    count+=1\n",
        "\n",
        "    if count>100:\n",
        "        break;"
      ],
      "metadata": {
        "id": "L6TmxPxtDVAT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_list_emb=[]\n",
        "for index, img_emb in enumerate(embedded_dataset[0]):\n",
        "    all_similarities=[]\n",
        "    for txt_emb in embedded_dataset[1]:\n",
        "        similarity = img_emb @ txt_emb.T\n",
        "        all_similarities.append(similarity.cpu().item())\n",
        "    max_similarity = np.argmax(all_similarities)\n",
        "\n",
        "    actual_image = embedded_dataset[2][index]\n",
        "    retrieved_image = embedded_dataset[2][max_similarity]\n",
        "\n",
        "    if (actual_image == retrieved_image):\n",
        "      all_list_emb.append(1)\n",
        "    else:\n",
        "      all_list_emb.append(0)\n",
        "\n",
        "print(\"Accuracy =\", np.mean(all_list_emb)100(1/100))"
      ],
      "metadata": {
        "id": "DKEtVajDTHuw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}