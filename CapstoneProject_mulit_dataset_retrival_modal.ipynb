{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNMh/e5yT+OjXsyzbKUqe9H",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/darkwingpatil/Ml_hackethons/blob/main/CapstoneProject_mulit_dataset_retrival_modal.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Multi media retrival  "
      ],
      "metadata": {
        "id": "wlQwGe9EcF54"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# For image download\n",
        "!wget https://github.com/jbrownlee/Datasets/releases/download/Flickr8k/Flickr8k_Dataset.zip\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ui0WsV6rcdTg",
        "outputId": "f54f4123-e41a-4a82-c4d3-aa2e2e1512ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-12-01 14:42:41--  https://github.com/jbrownlee/Datasets/releases/download/Flickr8k/Flickr8k_Dataset.zip\n",
            "Resolving github.com (github.com)... 140.82.113.3\n",
            "Connecting to github.com (github.com)|140.82.113.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://objects.githubusercontent.com/github-production-release-asset-2e65be/124585957/47f52b80-3501-11e9-8f49-4515a2a3339b?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=releaseassetproduction%2F20241201%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20241201T144242Z&X-Amz-Expires=300&X-Amz-Signature=8631b3262cfd05cb80f05ea1fb75dd6de293279531cbeee8e500ee416a11d740&X-Amz-SignedHeaders=host&response-content-disposition=attachment%3B%20filename%3DFlickr8k_Dataset.zip&response-content-type=application%2Foctet-stream [following]\n",
            "--2024-12-01 14:42:42--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/124585957/47f52b80-3501-11e9-8f49-4515a2a3339b?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=releaseassetproduction%2F20241201%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20241201T144242Z&X-Amz-Expires=300&X-Amz-Signature=8631b3262cfd05cb80f05ea1fb75dd6de293279531cbeee8e500ee416a11d740&X-Amz-SignedHeaders=host&response-content-disposition=attachment%3B%20filename%3DFlickr8k_Dataset.zip&response-content-type=application%2Foctet-stream\n",
            "Resolving objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115419746 (1.0G) [application/octet-stream]\n",
            "Saving to: ‘Flickr8k_Dataset.zip’\n",
            "\n",
            "Flickr8k_Dataset.zi 100%[===================>]   1.04G   233MB/s    in 4.2s    \n",
            "\n",
            "2024-12-01 14:42:46 (251 MB/s) - ‘Flickr8k_Dataset.zip’ saved [1115419746/1115419746]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# for caption dataset downlaod\n",
        "!wget https://github.com/jbrownlee/Datasets/releases/download/Flickr8k/Flickr8k_text.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E8EboKJ8g5K2",
        "outputId": "759951c7-a8af-4b33-974e-1b5297682a1f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-12-01 14:42:53--  https://github.com/jbrownlee/Datasets/releases/download/Flickr8k/Flickr8k_text.zip\n",
            "Resolving github.com (github.com)... 140.82.112.4\n",
            "Connecting to github.com (github.com)|140.82.112.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://objects.githubusercontent.com/github-production-release-asset-2e65be/124585957/47f52b80-3501-11e9-8d2e-dd69a21a4362?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=releaseassetproduction%2F20241201%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20241201T144253Z&X-Amz-Expires=300&X-Amz-Signature=cbc9d5da79da89842761441d6b885461fff6c7cd96f77f7f382b55f9d1887977&X-Amz-SignedHeaders=host&response-content-disposition=attachment%3B%20filename%3DFlickr8k_text.zip&response-content-type=application%2Foctet-stream [following]\n",
            "--2024-12-01 14:42:53--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/124585957/47f52b80-3501-11e9-8d2e-dd69a21a4362?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=releaseassetproduction%2F20241201%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20241201T144253Z&X-Amz-Expires=300&X-Amz-Signature=cbc9d5da79da89842761441d6b885461fff6c7cd96f77f7f382b55f9d1887977&X-Amz-SignedHeaders=host&response-content-disposition=attachment%3B%20filename%3DFlickr8k_text.zip&response-content-type=application%2Foctet-stream\n",
            "Resolving objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2340801 (2.2M) [application/octet-stream]\n",
            "Saving to: ‘Flickr8k_text.zip’\n",
            "\n",
            "Flickr8k_text.zip   100%[===================>]   2.23M  --.-KB/s    in 0.05s   \n",
            "\n",
            "2024-12-01 14:42:53 (45.0 MB/s) - ‘Flickr8k_text.zip’ saved [2340801/2340801]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "import os\n",
        "\n",
        "\n",
        "def unzip_file(zip_file_path, extract_to):\n",
        "    # Create the directory if it doesn't exist\n",
        "    # Path to the ZIP file\n",
        "    zip_file_path = zip_file_path\n",
        "\n",
        "    # Directory where you want to extract the files\n",
        "    extract_to = extract_to\n",
        "\n",
        "    # Create the directory if it doesn't exist\n",
        "    os.makedirs(extract_to, exist_ok=True)\n",
        "\n",
        "    # Open and extract the ZIP file\n",
        "    with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall(extract_to)\n",
        "\n",
        "    print(f\"Files extracted to: {extract_to}\")\n",
        "\n",
        "\n",
        "unzip_file(\"Flickr8k_Dataset.zip\",\"./Flicker8k_Image\")\n",
        "\n",
        "unzip_file(\"Flickr8k_text.zip\",\"./Flicker8k_Text\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UX1gUaQ9fsgT",
        "outputId": "8bcd63b6-745f-4378-abda-9446a7681515"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files extracted to: ./Flicker8k_Image\n",
            "Files extracted to: ./Flicker8k_Text\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# doing it using manual model\n",
        "# doing it using clip model\n"
      ],
      "metadata": {
        "id": "KNACS0YUgB7d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  Approach 2 using clip model\n",
        "#  Create an embedding pair using clip model"
      ],
      "metadata": {
        "id": "G7d5DjZYjTYe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch torchvision transformers ftfy regex\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uhBmQeZdkVtO",
        "outputId": "cdcee87b-3a51-4663-ffa4-7d3f400386a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.20.1+cu121)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.46.2)\n",
            "Collecting ftfy\n",
            "  Downloading ftfy-6.3.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (2024.9.11)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.26.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (11.0.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.26.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.6)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from ftfy) (0.2.13)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n",
            "Downloading ftfy-6.3.1-py3-none-any.whl (44 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.8/44.8 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: ftfy\n",
            "Successfully installed ftfy-6.3.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install git+https://github.com/openai/CLIP.git\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kRoJA_iPmUHb",
        "outputId": "ba6ae9bb-b406-46af-97d6-4fe7367231d7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/openai/CLIP.git\n",
            "  Cloning https://github.com/openai/CLIP.git to /tmp/pip-req-build-iff6pdfe\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/openai/CLIP.git /tmp/pip-req-build-iff6pdfe\n",
            "  Resolved https://github.com/openai/CLIP.git to commit dcba3cb2e2827b402d2701e7e1c7d9fed8a20ef1\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: ftfy in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (6.3.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (24.2)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (2024.9.11)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (4.66.6)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (2.5.1+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (0.20.1+cu121)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from ftfy->clip==1.0) (0.2.13)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch->clip==1.0) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision->clip==1.0) (1.26.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->clip==1.0) (11.0.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->clip==1.0) (3.0.2)\n",
            "Building wheels for collected packages: clip\n",
            "  Building wheel for clip (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for clip: filename=clip-1.0-py3-none-any.whl size=1369489 sha256=1b550ba40617d102af65e51c269fc9e12ecfe5ad22b1b4d0bf79534aae78e7e3\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-8yc89u9j/wheels/da/2b/4c/d6691fa9597aac8bb85d2ac13b112deb897d5b50f5ad9a37e4\n",
            "Successfully built clip\n",
            "Installing collected packages: clip\n",
            "Successfully installed clip-1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import clip\n",
        "from PIL import Image\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "\n",
        "class ImprovedClipDataset(Dataset):\n",
        "    def __init__(self, image_dir, labels_file, model, preprocess, device):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            image_dir (str): Path to the directory containing images.\n",
        "            labels_file (str): Path to the file with labels.\n",
        "            model: CLIP model\n",
        "            preprocess: CLIP preprocessing transform\n",
        "            device: Computation device\n",
        "        \"\"\"\n",
        "        self.image_dir = image_dir\n",
        "        self.model = model\n",
        "        self.preprocess = preprocess\n",
        "        self.device = device\n",
        "\n",
        "        # Parse labels file more robustly\n",
        "        self.image_labels = self._parse_labels_file(labels_file)\n",
        "\n",
        "    def _parse_labels_file(self, labels_file):\n",
        "        \"\"\"\n",
        "        Parse labels file with more robust handling\n",
        "        \"\"\"\n",
        "        image_labels = []\n",
        "        with open(labels_file, 'r', encoding='utf-8') as f:\n",
        "            for line in f:\n",
        "                parts = line.strip().split('\\t')\n",
        "                if len(parts) >= 2:\n",
        "                    img_name = parts[0]\n",
        "                    label = '\\t'.join(parts[1:])  # In case label contains tabs\n",
        "                    image_labels.append((img_name, label))\n",
        "        return image_labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_name, label = self.image_labels[idx]\n",
        "\n",
        "        # Handle image variations (removing #0, #1 etc.)\n",
        "        base_img_name = img_name.split('#')[0]\n",
        "        img_path = os.path.join(self.image_dir, base_img_name)\n",
        "\n",
        "        try:\n",
        "            # Open and preprocess image\n",
        "            image = Image.open(img_path).convert(\"RGB\")\n",
        "            processed_image = self.preprocess(image).unsqueeze(0).to(self.device)\n",
        "\n",
        "            # Tokenize text\n",
        "            text_token = clip.tokenize([label]).to(self.device)\n",
        "\n",
        "            return {\n",
        "                'img_name': img_name,\n",
        "                'processed_image': processed_image,\n",
        "                'text_token': text_token,\n",
        "                'original_label': label\n",
        "            }\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing {img_path}: {e}\")\n",
        "            return {\n",
        "                'img_name': img_name,\n",
        "                'processed_image': torch.zeros(1, 3, 224, 224, device=self.device), # Dummy image tensor\n",
        "                'text_token': clip.tokenize([\"\"]).to(self.device), # Dummy text token\n",
        "                'original_label': label\n",
        "            }\n",
        "\n",
        "def generate_embeddings(dataloader, model):\n",
        "    \"\"\"\n",
        "    Generate image and text embeddings in batches using DataLoader.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    image_embeddings = []\n",
        "    text_embeddings = []\n",
        "    image_names = []\n",
        "    original_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in dataloader:\n",
        "            if batch is None:\n",
        "                continue\n",
        "\n",
        "            # Generate image embedding\n",
        "            image_emb = model.encode_image(batch['processed_image'].squeeze(1))\n",
        "            image_emb = image_emb / image_emb.norm(dim=-1, keepdim=True)\n",
        "\n",
        "            # Generate text embedding\n",
        "            text_emb = model.encode_text(batch['text_token'].squeeze(1))\n",
        "            text_emb = text_emb / text_emb.norm(dim=-1, keepdim=True)\n",
        "\n",
        "            image_embeddings.append(image_emb.to(device))\n",
        "            text_embeddings.append(text_emb.to(device))\n",
        "            image_names.extend(batch['img_name'])\n",
        "            original_labels.extend(batch['original_label'])\n",
        "\n",
        "    return (\n",
        "        torch.cat(image_embeddings),\n",
        "        torch.cat(text_embeddings),\n",
        "        image_names,\n",
        "        original_labels\n",
        "    )\n",
        "\n",
        "\n",
        "def compute_similarity_matrix(image_embeddings, text_embeddings):\n",
        "    \"\"\"\n",
        "    Compute similarity matrix between image and text embeddings\n",
        "    \"\"\"\n",
        "    similarity_matrix = image_embeddings @ text_embeddings.T\n",
        "    return similarity_matrix\n",
        "\n",
        "# Example usage\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
        "\n",
        "image_path = \"Flicker8k_Image/Flicker8k_Dataset\"\n",
        "text_path = 'Flicker8k_Text/Flickr8k.token.txt'\n",
        "\n",
        "dataset = ImprovedClipDataset(\n",
        "    image_dir=image_path,\n",
        "    labels_file=text_path,\n",
        "    model=model,\n",
        "    preprocess=preprocess,\n",
        "    device=device\n",
        ")\n",
        "\n",
        "dataloader = DataLoader(dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "\n",
        "# Generate embeddings\n",
        "embedded_dataset = generate_embeddings(dataloader, model)\n"
      ],
      "metadata": {
        "id": "S9tJUMIlmKjc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(embedded_dataset[0]), len(embedded_dataset[1]), len(embedded_dataset[2]), len(embedded_dataset[3]))"
      ],
      "metadata": {
        "id": "whG9FrWE2pe6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "count=0;\n",
        "image_embeddings, text_embeddings, image_names, original_labels = embedded_dataset\n",
        "\n",
        "for i in range(len(image_embeddings)):\n",
        "    print(image_names[i], original_labels[i], \"static data\")\n",
        "    similarity = compute_similarity_matrix(image_embeddings[i],text_embeddings[i])\n",
        "    print(f\"Similarity Score: {similarity:.4f}\")\n",
        "    count+=1\n",
        "\n",
        "    if count>100:\n",
        "        break;"
      ],
      "metadata": {
        "id": "L6TmxPxtDVAT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}