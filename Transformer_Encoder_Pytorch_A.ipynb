{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/darkwingpatil/Ml_hackethons/blob/main/Transformer_Encoder_Pytorch_A.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hsglmGBRW8No"
      },
      "source": [
        "# Advanced Certification in AIML\n",
        "## A Program by IIIT-H and TalentSprint\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Learning Objectives\n",
        "\n",
        "At the end of the experiment, you will be able to:\n",
        "\n",
        "* understand the big picture of transformers\n",
        "* understand & implement positional embedding\n",
        "* understand & implement multi-head attention\n",
        "* understand & build transformer block by assembling individual components viz. positional encoding, multi-head attention, skip connection, layer normalzation\n",
        "* understand & build custom encoder transformer architecture\n",
        "* perform classification with custom build encoder transformer architecture using Huggingface built in dataset and tokenizer\n"
      ],
      "metadata": {
        "id": "aqlAEsqtWy_a"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qUmaxUW-rpxU"
      },
      "source": [
        "### Setup Steps:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XcrXxhfmrpxg"
      },
      "source": [
        "#@title Please enter your registration id to start: { run: \"auto\", display-mode: \"form\" }\n",
        "Id = \"2304145\" #@param {type:\"string\"}"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iDAfm706rpxg"
      },
      "source": [
        "#@title Please enter your password (normally your phone number) to continue: { run: \"auto\", display-mode: \"form\" }\n",
        "password = \"7892449987\" #@param {type:\"string\"}"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "KvdKENTWrpxh",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "d439df27-6e0c-4141-ea3d-068db5296731"
      },
      "source": [
        "#@title Run this cell to complete the setup for this Notebook\n",
        "from IPython import get_ipython\n",
        "import re\n",
        "ipython = get_ipython()\n",
        "\n",
        "notebook= \"U4W19_69_Tfr_Encoder_Pytorch_A\" #name of the notebook\n",
        "\n",
        "def setup():\n",
        "#  ipython.magic(\"sx pip3 install torch\")\n",
        "    from IPython.display import HTML, display\n",
        "    display(HTML('<script src=\"https://dashboard.talentsprint.com/aiml/record_ip.html?traineeId={0}&recordId={1}\"></script>'.format(getId(),submission_id)))\n",
        "    print(\"Setup completed successfully\")\n",
        "    return\n",
        "\n",
        "def submit_notebook():\n",
        "    ipython.magic(\"notebook -e \"+ notebook + \".ipynb\")\n",
        "\n",
        "    import requests, json, base64, datetime\n",
        "\n",
        "    url = \"https://dashboard.talentsprint.com/xp/app/save_notebook_attempts\"\n",
        "    if not submission_id:\n",
        "      data = {\"id\" : getId(), \"notebook\" : notebook, \"mobile\" : getPassword()}\n",
        "      r = requests.post(url, data = data)\n",
        "      r = json.loads(r.text)\n",
        "\n",
        "      if r[\"status\"] == \"Success\":\n",
        "          return r[\"record_id\"]\n",
        "      elif \"err\" in r:\n",
        "        print(r[\"err\"])\n",
        "        return None\n",
        "      else:\n",
        "        print (\"Something is wrong, the notebook will not be submitted for grading\")\n",
        "        return None\n",
        "\n",
        "    elif getAnswer() and getComplexity() and getAdditional() and getConcepts() and getWalkthrough() and getComments() and getMentorSupport():\n",
        "      f = open(notebook + \".ipynb\", \"rb\")\n",
        "      file_hash = base64.b64encode(f.read())\n",
        "\n",
        "      data = {\"complexity\" : Complexity, \"additional\" :Additional,\n",
        "              \"concepts\" : Concepts, \"record_id\" : submission_id,\n",
        "              \"answer\" : Answer, \"id\" : Id, \"file_hash\" : file_hash,\n",
        "              \"notebook\" : notebook, \"feedback_walkthrough\":Walkthrough ,\n",
        "              \"feedback_experiments_input\" : Comments,\n",
        "              \"feedback_inclass_mentor\": Mentor_support}\n",
        "\n",
        "      r = requests.post(url, data = data)\n",
        "      r = json.loads(r.text)\n",
        "      if \"err\" in r:\n",
        "        print(r[\"err\"])\n",
        "        return None\n",
        "      else:\n",
        "        print(\"Your submission is successful.\")\n",
        "        print(\"Ref Id:\", submission_id)\n",
        "        print(\"Date of submission: \", r[\"date\"])\n",
        "        print(\"Time of submission: \", r[\"time\"])\n",
        "        print(\"View your submissions: https://aiml-iiith.talentsprint.com/notebook_submissions\")\n",
        "        #print(\"For any queries/discrepancies, please connect with mentors through the chat icon in LMS dashboard.\")\n",
        "        return submission_id\n",
        "    else: submission_id\n",
        "\n",
        "\n",
        "def getAdditional():\n",
        "  try:\n",
        "    if not Additional:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Additional\n",
        "  except NameError:\n",
        "    print (\"Please answer Additional Question\")\n",
        "    return None\n",
        "\n",
        "def getComplexity():\n",
        "  try:\n",
        "    if not Complexity:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Complexity\n",
        "  except NameError:\n",
        "    print (\"Please answer Complexity Question\")\n",
        "    return None\n",
        "\n",
        "def getConcepts():\n",
        "  try:\n",
        "    if not Concepts:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Concepts\n",
        "  except NameError:\n",
        "    print (\"Please answer Concepts Question\")\n",
        "    return None\n",
        "\n",
        "\n",
        "def getWalkthrough():\n",
        "  try:\n",
        "    if not Walkthrough:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Walkthrough\n",
        "  except NameError:\n",
        "    print (\"Please answer Walkthrough Question\")\n",
        "    return None\n",
        "\n",
        "def getComments():\n",
        "  try:\n",
        "    if not Comments:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Comments\n",
        "  except NameError:\n",
        "    print (\"Please answer Comments Question\")\n",
        "    return None\n",
        "\n",
        "\n",
        "def getMentorSupport():\n",
        "  try:\n",
        "    if not Mentor_support:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Mentor_support\n",
        "  except NameError:\n",
        "    print (\"Please answer Mentor support Question\")\n",
        "    return None\n",
        "\n",
        "def getAnswer():\n",
        "  try:\n",
        "    if not Answer:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Answer\n",
        "  except NameError:\n",
        "    print (\"Please answer Question\")\n",
        "    return None\n",
        "\n",
        "\n",
        "def getId():\n",
        "  try:\n",
        "    return Id if Id else None\n",
        "  except NameError:\n",
        "    return None\n",
        "\n",
        "def getPassword():\n",
        "  try:\n",
        "    return password if password else None\n",
        "  except NameError:\n",
        "    return None\n",
        "\n",
        "submission_id = None\n",
        "### Setup\n",
        "if getPassword() and getId():\n",
        "  submission_id = submit_notebook()\n",
        "  if submission_id:\n",
        "    setup()\n",
        "else:\n",
        "  print (\"Please complete Id and Password cells before running setup\")\n",
        "\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<script src=\"https://dashboard.talentsprint.com/aiml/record_ip.html?traineeId=2304145&recordId=8913\"></script>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setup completed successfully\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Building Encoder Transformer**"
      ],
      "metadata": {
        "id": "TcPgAXqQ1IEI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Introduction**\n",
        "Figure below shows the **Transformer Model Architecture** as per the paper [\"**Attention Is All You Need**\"](https://arxiv.org/pdf/1706.03762v6.pdf) .Here, we are going to implement **Encoder Transformer** and try to understand how it function. We are writing **'as per the paper'** to  mention this paper throughout this notebook.\n",
        "\n",
        "To completely understand the Encoder Transformer, it is imperative to understand Self Attention  ( [Details of Self-Attention](https://colab.research.google.com/drive/1fvoLK4zk1C_yA1aRYg1K1LQJ2ivAJQMk?usp=sharing)) which is used inside Multi-head Attention. The data after passing the TextVectorization layer and Embedding layer will pass the Self Attention layer.\n",
        "<br><br>\n",
        "<center>\n",
        "<img src= \"https://cdn.extras.talentsprint.com/aiml/Experiment_related_data/Images/Transformer.png\" width=750px/>\n",
        "\n",
        "</center>"
      ],
      "metadata": {
        "id": "6tlFf8Vd1SdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Multihead Attention**\n",
        "\n",
        "In the Transformer, the Attention module repeats its computations multiple times in parallel. Each of these is called an Attention Head. The Attention module splits its Query, Key, and Value parameters N-ways and passes each split independently through a separate Head. All of these similar Attention calculations are then combined together to produce a final Attention score. This is called Multi-head attention and gives the Transformer greater power to encode multiple relationships and nuances for each word.\n",
        "\n"
      ],
      "metadata": {
        "id": "fcSJEufB1_Ye"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the paper, the diagram for a scaled dot product attention does not use any weights at all. Instead, the weights are included only in the multi head attention block, shown in figure below :\n",
        "<br><br>\n",
        "<center>\n",
        "<img src= \"https://cdn.extras.talentsprint.com/aiml/Experiment_related_data/Images/Multi_head_attention_with_weights.png\" width=900px/>\n",
        "</center>"
      ],
      "metadata": {
        "id": "-d_O7l_w3BuL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Understanding the shape\n",
        "<center>\n",
        "<img src= \"https://cdn.extras.talentsprint.com/aiml/Experiment_related_data/Images/Multi_head_attention_shape_tracking.png\" width=800px/>\n",
        "</center>\n",
        "\n",
        "**Final Projection :** $ Output = concat(A_1, A_2, ..., A_h)  W^{o} $\n",
        "\n",
        "**Shape of :**  $ concat (A_1, A_2, ..., A_h) \\Rightarrow  (T \\times hd_v) $\n",
        "\n",
        "**Shape of:**  $  W^{o} \\Rightarrow (hd_v \\times d_{model}) $\n",
        "\n",
        "**Shape of final:**  $ Ouput = concat (A_1, A_2, ..., A_h) W^{o} \\Rightarrow  (T \\times hd_v) \\times (hd_v \\times d_{model})  \\Rightarrow  (T \\times d_{model}) \\Leftarrow $ **Back to the initial input shape.**\n",
        "\n",
        "Note : Batch size is not displayed here."
      ],
      "metadata": {
        "id": "sNSqerZp3KRn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Importing packages"
      ],
      "metadata": {
        "id": "0bcpjWqaYpJO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "nNlcrwNl3xAG"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import dataset\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Implementation of Multihead Attention**"
      ],
      "metadata": {
        "id": "VT1uPdfA2xGC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# https://chatgpt.com/c/671ced41-3ff8-800f-876f-ea60d4a29fc2  <- INTUATION FOR GPT CHAT\n",
        "class MultiHeadAttention(nn.Module):\n",
        "  def __init__(self, d_k,d_model, n_heads):\n",
        "    super().__init__()\n",
        "\n",
        "    # Assume d_v =d_k\n",
        "    self.d_k=d_k\n",
        "    self.n_heads = n_heads\n",
        "\n",
        "    self.key = nn. Linear (d_model, d_k*n_heads)\n",
        "    self.query = nn. Linear (d_model, d_k*n_heads)\n",
        "    self.value = nn. Linear (d_model, d_k*n_heads)\n",
        "\n",
        "    # final linear layer\n",
        "    self.fc=nn.Linear (d_k*n_heads, d_model) # 8*8\n",
        "\n",
        "  def forward(self, q,k,v, mask=None):\n",
        "    q=self.query(q) # N x T x (hd_k)  (2, 3, 8)\n",
        "    k=self.key(k) # N x T x (hd_k)  (2, 3, 8)\n",
        "    v=self.value(v) # N x T x (hd_k) (2, 3, 8)\n",
        "\n",
        "    N=q.shape[0]\n",
        "    T=q.shape[1]\n",
        "\n",
        "    # change the shape to:\n",
        "    # (N, T, h, d_k) --> N, h, T, d_k)\n",
        "    # in order for matrix multiply to work properly\n",
        "    q=q.view (N, T, self.n_heads, self.d_k).transpose(1,2)  # (2, 3, 2, 4) -> (2, 2, 3, 4)\n",
        "    k=k.view (N, T, self.n_heads, self.d_k).transpose(1,2)  #(2, 3, 2, 4) -> (2, 2, 3, 4)\n",
        "    v=v.view (N, T, self.n_heads, self.d_k).transpose(1,2)  #(2, 3, 2, 4) -> (2, 2, 3, 4)\n",
        "\n",
        "    # Copute attention weights\n",
        "    # (N, h, T, d_k)   x  (N, h, d_k, T )  --> (N, h, T, T)\n",
        "    attn_scores = q@k.transpose(-2,-1)/math.sqrt(self.d_k) # Scaled dot product;  @ --> torch.matmul\n",
        "    if mask is not None:\n",
        "      attn_scores = attn_scores.masked_fill(mask[:,None,None,:]==0, float('-inf'))\n",
        "    attn_weights = F.softmax(attn_scores, dim =-1)\n",
        "\n",
        "    # Compute attention-weighted values\n",
        "    # (N, h, T, T) x (N, h, T, d_k) --> (N, h, T, d_k)\n",
        "    A = attn_weights @ v  # (2,2,3,4)\n",
        "\n",
        "    # reshape it back before final linear layer\n",
        "    A = A.transpose(1,2)  # (N, T, h, d_k)  # (2,3,2,4)\n",
        "    A = A.contiguous(). view(N, T, self.d_k*self.n_heads) # (N, T, h*d_k)  # (2,3,8)\n",
        "\n",
        "    # projection\n",
        "    return self.fc(A)\n",
        "\n",
        ""
      ],
      "metadata": {
        "id": "mpwJtf-I4Uky"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Transformer Block**\n",
        "\n",
        "The Transformer Encoder consists of a stack of\n",
        " identical layers (Encoder Block) as shown in figure below, where each layer further consists of two main sub-layers:\n",
        "\n",
        "* The first sub-layer comprises a multi-head attention mechanism that receives the queries, keys, and values as inputs.\n",
        "* A second sub-layer comprises a fully-connected feed-forward network.\n",
        "\n",
        "Following each of these two sub-layers is layer normalization, into which the sub-layer input (through a residual/skip connection) and output are fed.\n",
        "\n",
        "Regularization is also intrpduced into the model by applying a dropout to the output of each sub-layer (before the layer normalization step), as well as to the positional encodings before these are fed into the encoder.\n",
        "\n",
        "\n",
        "\n",
        "<br>\n",
        "<center>\n",
        "<img src=\"https://cdn.extras.talentsprint.com/aiml/Experiment_related_data//Images/Encoder_tfr_block_unfolded.png\"  width=600 px />$⇒$\n",
        "<img src=\"https://cdn.extras.talentsprint.com/aiml/Experiment_related_data/Images/Encoder_tfr_block.png\" width=180 px/>\n",
        "\n",
        "</center>"
      ],
      "metadata": {
        "id": "dmLespmb39pi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The transformer encoder architecture typically consists of multiple layers, each of which includes a self-attention mechanism and a feed-forward neural network. The self-attention mechanism allows the model to weigh the importance of different input sequence parts by calculating the embeddings' dot product. This mechanism is also known as multi-head attention.\n",
        "\n",
        "The feed-forward network allows the model to extract higher-level features from the input. This network usually comprises two linear layers with a ReLU activation function in between. The feed-forward network allows the model to extract deeper meaning from the input data and more compactly and usefully represent the input.In the paper, an ANN with one hidden layer and a ReLu activation in the middle  with no activation function at output layer has been implemented.\n",
        "\n",
        "The transformer encoder is a crucial part of the transformer encoder-decoder architecture, which is widely used for natural language processing tasks."
      ],
      "metadata": {
        "id": "DV1-ekc54J4k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Implementation of Transformder Block**"
      ],
      "metadata": {
        "id": "AV2O1rWO23dK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerBlock(nn.Module):\n",
        "  def __init__(self, d_k, d_model, n_heads, dropout_prob = 0.1):\n",
        "    super().__init__()\n",
        "\n",
        "    self.ln1 = nn.LayerNorm(d_model)\n",
        "    self.ln2 = nn.LayerNorm(d_model)\n",
        "    self.mha = MultiHeadAttention(d_k, d_model, n_heads)\n",
        "\n",
        "    self.ann = nn.Sequential(\n",
        "        nn.Linear (d_model, d_model*4),\n",
        "        nn.GELU(),\n",
        "        nn.Linear(d_model*4, d_model),\n",
        "        nn.Dropout(dropout_prob),\n",
        "    )\n",
        "\n",
        "    self.dropout = nn. Dropout(p=dropout_prob)\n",
        "\n",
        "  def forward (self, x, mask=None):\n",
        "    #  x =(2,3,8)\n",
        "    x = self.ln1(x + self.mha(x, x, x, mask))\n",
        "    x = self.ln2(x + self.ann(x))\n",
        "    x = self.dropout(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "4xQPfsMF4UqZ"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Positional Embedding**\n",
        "\n",
        "**Positional Embedding = Word Embedding + Positional Encoding**\n",
        "\n",
        "**Positional Encoding**\n",
        "\n",
        "Passing embeddings directly into the transformer block results in missing of information about the order of tokens. As attention is permutation invariant i.e. order of token does not matter to attention.\n",
        "Although transformers are a sequence model, it appears that this important detail has somehow been lost. Positional encoding is for rescue.\n",
        "\n",
        "Positional encoding add positional information to the existing embeddings.\n",
        "\n",
        "**A unique set of numbers added at each position of the existing embeddings**, such that this new set of numbers can uniquely identify which postion they are located at. Following two ways are there to add positional encoding:\n",
        "\n",
        "\n",
        "1. Positional Encoding by SubClassing the Embedding Layer (Trainable)\n",
        "2. Positional Encoding scheme as per the paper (Non-Trainable)\n",
        "\n",
        " In this scheme the encoding is created by using a set of sins and cosines at different frequencies. The  paper uses the following formula for calculating the positional encoding. [Positional Encoding Vizualization.](https://erdem.pl/2021/05/understanding-positional-encoding-in-transformers)\n",
        "\n",
        "$$\\ {PE_{(pos, 2i)} = \\sin(pos / 10000^{2i / d_{model}})} $$\n",
        "$$\\ {PE_{(pos, 2i+1)} = \\cos(pos / 10000^{2i / d_{model}})} $$\n",
        "\n",
        "We are going to implement Positional Encoding as per the paper."
      ],
      "metadata": {
        "id": "vAj2KspZ4V-L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Implementation of Positional Encoding**"
      ],
      "metadata": {
        "id": "HaGRSrHu322S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "  def __init__(self, d_model, max_len =2048, dropout_porb=0.1):\n",
        "    super().__init__()\n",
        "    self.dropout = nn.Dropout(p = dropout_porb)\n",
        "\n",
        "    position = torch.arange(max_len).unsqueeze(1)\n",
        "    exp_term = torch.arange(0, d_model, 2)\n",
        "    div_term = torch.exp(exp_term*(-math.log(10000.0)/d_model))\n",
        "    pe = torch.zeros(1, max_len, d_model)\n",
        "    pe[0, :, 0::2] = torch.sin(position *div_term)\n",
        "    pe[0, :, 1::2] = torch.cos(position *div_term)\n",
        "    self.register_buffer('pe', pe)\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "    # x.shape : N x T x D\n",
        "    x=x + self.pe[:, :x.size(1), :]\n",
        "    return self.dropout(x)\n"
      ],
      "metadata": {
        "id": "ILH0hVaJ4UtI"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Encoder Transformer**\n",
        "Stacking transormer blocks gives a Transfomer! Shown in figure below:\n",
        "\n",
        "<br>\n",
        "<center>\n",
        "<img src= \"https://cdn.extras.talentsprint.com/aiml/Experiment_related_data/Images/Encoder_transfomer.png\" width=1000px/>\n",
        "</center>"
      ],
      "metadata": {
        "id": "gTt6oHcs4foM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Implementation of Encoder**\n",
        "\n",
        "Define Transformer Encoder class"
      ],
      "metadata": {
        "id": "6t1VKvaM64nG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "  def __init__(self, vocab_size, max_len, d_k, d_model, n_heads, n_layers, n_classes, dropout_prob):\n",
        "    super().__init__()\n",
        "\n",
        "    self.embedding=nn.Embedding(vocab_size, d_model)\n",
        "    self.pos_encoding=PositionalEncoding(d_model, max_len, dropout_prob)\n",
        "    transformer_blocks=[TransformerBlock(d_k, d_model, n_heads, dropout_prob) for _ in range(n_layers) ]\n",
        "    self.transformer_blocks = nn.Sequential(*transformer_blocks)\n",
        "    self.ln = nn.LayerNorm(d_model)\n",
        "    self.fc = nn.Linear(d_model, n_classes)\n",
        "\n",
        "  def forward(self, x, mask = None):\n",
        "    x=self.embedding(x)\n",
        "    x=self.pos_encoding(x)\n",
        "    for block in self.transformer_blocks:\n",
        "      x = block( x, mask)\n",
        "\n",
        "    # many to one (x has the shape N x T x D)\n",
        "    x = x[:, 0, :]\n",
        "    x = self.ln(x)\n",
        "    x = self.fc(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "e9Y9HGjm4Uvj"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Testing the forward pass with dummy values**"
      ],
      "metadata": {
        "id": "vvNgO6s_Z2sk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = Encoder(20_000, 1024, 16, 64, 4, 2, 5, 0.1)\n",
        "#vocab_size =20_000, max_length = 1024, d_k=16, d_model = 64, n_heads=4, nlayers =2, n_classes =5, dropout_prob=0.1"
      ],
      "metadata": {
        "id": "uSxOLuPD4UyA"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device (\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)\n",
        "model.to(device)"
      ],
      "metadata": {
        "id": "Botw-iWz4U0R",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "db293b47-109a-4944-924e-3ea318eb9b36"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda:0\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Encoder(\n",
              "  (embedding): Embedding(20000, 64)\n",
              "  (pos_encoding): PositionalEncoding(\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              "  (transformer_blocks): Sequential(\n",
              "    (0): TransformerBlock(\n",
              "      (ln1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
              "      (ln2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
              "      (mha): MultiHeadAttention(\n",
              "        (key): Linear(in_features=64, out_features=64, bias=True)\n",
              "        (query): Linear(in_features=64, out_features=64, bias=True)\n",
              "        (value): Linear(in_features=64, out_features=64, bias=True)\n",
              "        (fc): Linear(in_features=64, out_features=64, bias=True)\n",
              "      )\n",
              "      (ann): Sequential(\n",
              "        (0): Linear(in_features=64, out_features=256, bias=True)\n",
              "        (1): GELU(approximate='none')\n",
              "        (2): Linear(in_features=256, out_features=64, bias=True)\n",
              "        (3): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (1): TransformerBlock(\n",
              "      (ln1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
              "      (ln2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
              "      (mha): MultiHeadAttention(\n",
              "        (key): Linear(in_features=64, out_features=64, bias=True)\n",
              "        (query): Linear(in_features=64, out_features=64, bias=True)\n",
              "        (value): Linear(in_features=64, out_features=64, bias=True)\n",
              "        (fc): Linear(in_features=64, out_features=64, bias=True)\n",
              "      )\n",
              "      (ann): Sequential(\n",
              "        (0): Linear(in_features=64, out_features=256, bias=True)\n",
              "        (1): GELU(approximate='none')\n",
              "        (2): Linear(in_features=256, out_features=64, bias=True)\n",
              "        (3): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "  )\n",
              "  (ln): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
              "  (fc): Linear(in_features=64, out_features=5, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = np.random.randint(0, 20_000, size = (8,512))\n",
        "x_t = torch.tensor(x).to(device )"
      ],
      "metadata": {
        "id": "52GN7WHu4U2p"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mask = np.ones((8,512))\n",
        "mask[:, 256:] = 0\n",
        "mask_t = torch.tensor (mask).to (device)"
      ],
      "metadata": {
        "id": "yPOXCKXl4U47"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y = model(x_t, mask_t)"
      ],
      "metadata": {
        "id": "k3qRUdWw-Ah7"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y.shape"
      ],
      "metadata": {
        "id": "C4NrbK2xA-Q2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5e377fef-98c6-4b7b-aa7d-04cb6c52dbf0"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([8, 5])"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Train and evaluate the Encoder using a real dataset**\n",
        "\n",
        "We are going to use model built above (custom  encoder architecture) for sentiment classificaiton. Huggingface Library is used for demonstration using built in dataset **`glue - sst2`** and tokenizer."
      ],
      "metadata": {
        "id": "XR53ahXLDvQ6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Installing transformers and dataset packages"
      ],
      "metadata": {
        "id": "FITbsHZI9_3b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers datasets"
      ],
      "metadata": {
        "id": "jjb57U7iC00c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, DataCollatorWithPadding"
      ],
      "metadata": {
        "id": "lEt65o4sC6tj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint = 'distilbert-base-cased'\n",
        "tokenizer = # YOUR CODE HERE : load the pretrained weights from the distil bert model"
      ],
      "metadata": {
        "id": "4SO_znnhDG6l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset"
      ],
      "metadata": {
        "id": "ug68vXm8DUKl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "raw_datasets = load_dataset(\"glue\", \"sst2\") # sst2 is DataSet for sentiment analysis as a part of glue bench-mark"
      ],
      "metadata": {
        "id": "dN55LilBDUPg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "raw_datasets"
      ],
      "metadata": {
        "id": "3bPkcCCdDUTB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "raw_datasets['train'].features"
      ],
      "metadata": {
        "id": "g9XdK-DJY9dI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "raw_datasets['train']"
      ],
      "metadata": {
        "id": "wq97kh4kIhZt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dir(raw_datasets['train'])"
      ],
      "metadata": {
        "id": "DIm3zznKXqeR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "type(raw_datasets['train'])"
      ],
      "metadata": {
        "id": "q0VMtsfGXyBO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "raw_datasets['train'].data"
      ],
      "metadata": {
        "id": "l8rReSo5Xc_g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "raw_datasets['train'][1]"
      ],
      "metadata": {
        "id": "yAUPAiWIX05M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "raw_datasets['train'][10:14]"
      ],
      "metadata": {
        "id": "FhQspc65YBXF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "raw_datasets['train'][0:3]['sentence']"
      ],
      "metadata": {
        "id": "R44px4o5aNOG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_sentences=tokenizer('hide new secretions from the parental units ')#(raw_datasets['train'][0:3]['sentence'])\n",
        "from pprint import pprint\n",
        "pprint(tokenized_sentences)"
      ],
      "metadata": {
        "id": "Sas1Ab99Y1zn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_fn(batch):\n",
        "  return tokenizer(batch['sentence'], truncation = True)"
      ],
      "metadata": {
        "id": "vg8A-qgLEPNM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_datasets = # YOUR CODE HERE : To map the raw_datasets with tokenize_fn\n",
        "data_collator = DataCollatorWithPadding(tokenizer = tokenizer)"
      ],
      "metadata": {
        "id": "9zUORf97EmXw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_collator"
      ],
      "metadata": {
        "id": "9ZBK5YnfE3AY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lets look at our tokenized data sets --> some new columns have been added.\n",
        "\n",
        "There are now two new columns called Input IDs and Attention Mask. Input IDs are the token indices and Attention Mask tells us which tokens are real tokens and which are padding."
      ],
      "metadata": {
        "id": "b4MbXffmA9SD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_datasets"
      ],
      "metadata": {
        "id": "pimTj4VnE5QY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# YOUR CODE HERE : Remove the columns sentence and idx\n",
        "# YOUR CODE HERE : Rename the column label to labels"
      ],
      "metadata": {
        "id": "c56S2MNJFHor"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_datasets"
      ],
      "metadata": {
        "id": "efzICOsDJi_o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader"
      ],
      "metadata": {
        "id": "--mqwOoGGxpH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader =  # YOUR CODE HERE : Load the DataLoader for train set with respective parameters\n",
        "valid_loader = # YOUR CODE HERE : Load the DataLoader for validation set with respective parameters"
      ],
      "metadata": {
        "id": "3jtjWlymG3BP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check how it works\n",
        "for batch in train_loader:\n",
        "  for k,v in batch.items():\n",
        "    print(\"k:\", k, \"v.shape:\", v.shape)\n",
        "  break"
      ],
      "metadata": {
        "id": "Xn5_FTDZH7gF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "set(tokenized_datasets['train']['labels']) # Gives classes"
      ],
      "metadata": {
        "id": "7J_JmP19Jybh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.vocab_size"
      ],
      "metadata": {
        "id": "Prp0P9HhJtm5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.model_max_length"
      ],
      "metadata": {
        "id": "_TUSW_eGJ6bL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = # YOUR CODE HERE : Call the model with required hyperparameters\n",
        "model.to(device)"
      ],
      "metadata": {
        "id": "bQsxpZ6ISX8j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = # YOUR CODE HERE : Loss function\n",
        "optimizer = # YOUR CODE HERE : Optimizer with model parameters"
      ],
      "metadata": {
        "id": "-2sMrP9OSXme"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# A function to encapsulate the training loop"
      ],
      "metadata": {
        "id": "wsC0JWUeTjQ0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datetime import datetime"
      ],
      "metadata": {
        "id": "eZoL1jSxUGH_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, criterion, optimzer, train_loader, valid_loader, epochs):\n",
        "  train_losses = np.zeros(epochs)\n",
        "  test_losses = np.zeros(epochs)\n",
        "\n",
        "  for it in range(epochs):\n",
        "    model.train()\n",
        "    t0=datetime.now()\n",
        "    train_loss =0\n",
        "    n_train =0\n",
        "    for batch in train_loader:\n",
        "      # YOUr CODE HERE : To Train the model on train data\n",
        "    # get average train Loss\n",
        "    train_loss =train_loss/n_train\n",
        "\n",
        "    model.eval()\n",
        "    test_loss=0\n",
        "    n_test=0\n",
        "    for batch in valid_loader:\n",
        "        # YOUR CODE HERE : To Evaluate the model on Validation data\n",
        "    test_loss = test_loss/n_test\n",
        "\n",
        "    # save lossess\n",
        "    train_losses[it] = train_loss\n",
        "    test_losses[it] = test_loss\n",
        "\n",
        "    dt = datetime.now() -t0\n",
        "    print(f'Epoch{it+1}/{epochs}, Train Loss: {train_loss:.4f}, Test Loss : {test_loss:.4f}, Duration {dt}')\n",
        "\n",
        "  return train_losses, test_losses"
      ],
      "metadata": {
        "id": "m7RxlS2kTnqc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_losses, test_losses = train(model, criterion, optimizer, train_loader, valid_loader, epochs =4)"
      ],
      "metadata": {
        "id": "yYK-p8w9XXQW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Accuracy\n",
        "model.eval()\n",
        "n_correct = 0.\n",
        "n_total = 0.\n",
        "\n",
        "for batch in train_loader:\n",
        "   # Move to GPU\n",
        "\n",
        "   batch ={k: v.to(device) for k,v in batch.items()}\n",
        "\n",
        "   # Forward pass\n",
        "   outputs = model (batch['input_ids'], batch['attention_mask'])\n",
        "\n",
        "   # Get prediction\n",
        "   # torch.max returns oth max and argmax\n",
        "   _, predictions =torch.max(outputs,1)\n",
        "   # update counts\n",
        "   n_correct += (predictions == batch['labels']).sum().item()\n",
        "   n_total += batch['labels'].shape[0]\n",
        "\n",
        "train_acc = n_correct/n_total\n",
        "\n",
        "n_correct = 0.\n",
        "n_total = 0.\n",
        "\n",
        "for batch in valid_loader:\n",
        "   # Move to GPU\n",
        "\n",
        "   batch ={k: v.to(device) for k,v in batch.items()}\n",
        "\n",
        "   # Forward pass\n",
        "   outputs = model (batch['input_ids'], batch['attention_mask'])\n",
        "\n",
        "   # Get prediction\n",
        "   # torch.max returns oth max and argmax\n",
        "   _, predictions =torch.max(outputs,1)\n",
        "   # update counts\n",
        "   n_correct += (predictions == batch['labels']).sum().item()\n",
        "   n_total += batch['labels'].shape[0]\n",
        "\n",
        "test_acc = n_correct/n_total\n"
      ],
      "metadata": {
        "id": "NM7RkIQDXtxB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_acc, test_acc"
      ],
      "metadata": {
        "id": "gZZelQpFivsK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## References\n",
        "\n",
        "If you are very interested or plan to work closely on Transformers, then following are good resource that explains in simplified manner.\n",
        "\n",
        "1.[Illustrated-transformer](https://jalammar.github.io/illustrated-transformer/)\n",
        "\n",
        "2.[Understanding Positional  Encoding](https://erdem.pl/2021/05/understanding-positional-encoding-in-transformers)\n",
        "\n",
        "3.[Attention Is All You Need](https://arxiv.org/pdf/1706.03762v6.pdf)"
      ],
      "metadata": {
        "id": "nb6_h4LK7u12"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I2bjwSE88WSV"
      },
      "source": [
        "### Please answer the questions below to complete the experiment:\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f7vfJLjr8WSo"
      },
      "source": [
        "#@title Which of the following implementations facilitates parallel operation in transformer architecture?  { run: \"auto\", form-width: \"500px\", display-mode: \"form\" }\n",
        "Answer = \"Multi-head Attention\" #@param [\"\",\"Positional Encoding\", \"Skip Connection and Layer Normalization\", \"Feed Forward ANN\", \"Multi-head Attention\"]\n"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IcivJ9dk8WSp"
      },
      "source": [
        "#@title How was the experiment? { run: \"auto\", form-width: \"500px\", display-mode: \"form\" }\n",
        "Complexity = \"Good and Challenging for me\" #@param [\"\",\"Too Simple, I am wasting time\", \"Good, But Not Challenging for me\", \"Good and Challenging for me\", \"Was Tough, but I did it\", \"Too Difficult for me\"]\n"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yc6jZtlD8WSq"
      },
      "source": [
        "#@title If it was too easy, what more would you have liked to be added? If it was very difficult, what would you have liked to have been removed? { run: \"auto\", display-mode: \"form\" }\n",
        "Additional = \"na\" #@param {type:\"string\"}\n"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4w6tiLP78WSq"
      },
      "source": [
        "#@title Can you identify the concepts from the lecture which this experiment covered? { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n",
        "Concepts = \"Yes\" #@param [\"\",\"Yes\", \"No\"]\n"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UysV38w28WSq"
      },
      "source": [
        "#@title  Experiment walkthrough video? { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n",
        "Walkthrough = \"Didn't use\" #@param [\"\",\"Very Useful\", \"Somewhat Useful\", \"Not Useful\", \"Didn't use\"]\n"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B94HZp1B8WSr"
      },
      "source": [
        "#@title  Text and image description/explanation and code comments within the experiment: { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n",
        "Comments = \"Somewhat Useful\" #@param [\"\",\"Very Useful\", \"Somewhat Useful\", \"Not Useful\", \"Didn't use\"]\n"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_rrCdpRi8WSs"
      },
      "source": [
        "#@title Mentor Support: { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n",
        "Mentor_support = \"Didn't use\" #@param [\"\",\"Very Useful\", \"Somewhat Useful\", \"Not Useful\", \"Didn't use\"]\n"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "zeG71xha8WSs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3fe50949-13e5-4af7-863f-c982ac6a3158"
      },
      "source": [
        "#@title Run this cell to submit your notebook for grading { vertical-output: true }\n",
        "try:\n",
        "  if submission_id:\n",
        "      return_id = submit_notebook()\n",
        "      if return_id : submission_id = return_id\n",
        "  else:\n",
        "      print(\"Please complete the setup first.\")\n",
        "except NameError:\n",
        "  print (\"Please complete the setup first.\")"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Your submission is successful.\n",
            "Ref Id: 8913\n",
            "Date of submission:  26 Oct 2024\n",
            "Time of submission:  19:54:40\n",
            "View your submissions: https://aiml-iiith.talentsprint.com/notebook_submissions\n"
          ]
        }
      ]
    }
  ]
}